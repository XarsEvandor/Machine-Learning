{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"5.5 DNN for Multiclass Classification with Keras.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPKHW0YFwjdHsZllCka0dy+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 5.5 Training a Deep Neural Network for multiclass classification\n","This examples illustrates a basic DNN in Tensorflow/Keras that is trained for a multiclass classification task. It has the ability to create a variable count of layers, given a python list of the neuron count in each layer. The activation function for hidden layers is the **Rectified Linear Unit (ReLU)** and for the output layer is the **Softmax**. In the output each neuron is a proper class probability and the sum of all neuron outputs is 1."],"metadata":{"id":"DojV2IqZcaom"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"I0m8s-KccaJp"},"outputs":[],"source":["# Mount GDrive, change directory and check contents of folder.\n","\n","import os\n","from google.colab import drive\n","from google.colab import files\n","\n","PROJECT_FOLDER = \"/content/gdrive/My Drive/Colab Notebooks/CS345_SP22/5. DNN\"\n","\n","drive.mount('/content/gdrive/')\n","os.chdir(PROJECT_FOLDER)\n","print(\"Current dir: \", os.getcwd())"]},{"cell_type":"markdown","source":["# Settings and Basic Package Imports"],"metadata":{"id":"y0TJgTCXbfrZ"}},{"cell_type":"code","source":["import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow import keras\n","from mllib.utils import RandomSeed\n","\n","# __________ | Settings | __________\n","IS_PLOTING_DATA         = True\n","IS_DEBUGABLE            = False\n","IS_RETRAINING           = True\n","RandomSeed(2022)\n","\n","sColorScheme = [\"darkseagreen\", \"royalblue\", \"crimson\", \"goldenrod\"] # https://matplotlib.org/3.1.0/gallery/color/named_colors.html"],"metadata":{"id":"kIrldDF0bgGt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Hyperparameters\n","For each training experiment, we define all the model/training hyperparameters inside a Python dictionary."],"metadata":{"id":"5z0Jm3SFxJWv"}},{"cell_type":"code","source":["# 2 Layers -> 22180 Parameters -> 0.95 Accuracy\n","CONFIG_BASELINE = {\n","            \"ModelName\": \"QPEDS2\"  \n","           ,\"DNN.InputFeatures\": 72\n","           ,\"DNN.LayerNeurons\": [288, 4]\n","           ,\"DNN.Classes\": 4\n","           ,\"Training.MaxEpoch\": 200\n","           ,\"Training.BatchSize\": 160\n","           ,\"Training.LearningRate\": 0.2\n","          }\n","\n","# 3 Layers -> 10804 Parameters -> 0.97 Accuracy\n","CONFIG_GOOD_3 = {\n","            \"ModelName\": \"QPEDS3\"  \n","           ,\"DNN.InputFeatures\": 72\n","           ,\"DNN.LayerNeurons\": [72,72,4]\n","           ,\"DNN.Classes\": 4\n","           ,\"Training.MaxEpoch\": 400\n","           ,\"Training.BatchSize\": 160\n","           ,\"Training.LearningRate\": 0.1\n","          }\n","\n","\n","\n","# 4 Layers -> 1780 Parameters -> 0.98 Accuracy\n","CONFIG_BEST_4 = {\n","            \"ModelName\": \"QPEDS4\"  \n","           ,\"DNN.InputFeatures\": 72\n","           ,\"DNN.LayerNeurons\": [16,16,16,4]\n","           ,\"DNN.Classes\": 4\n","           ,\"Training.MaxEpoch\": 400\n","           ,\"Training.BatchSize\": 160\n","           ,\"Training.LearningRate\": 0.1\n","          }\n","\n","# 5 Layers -> 2052 Parameters -> 0.98 Accuracy\n","CONFIG_BEST_5 = {\n","            \"ModelName\": \"QPEDS5\"  \n","           ,\"DNN.InputFeatures\": 72\n","           ,\"DNN.LayerNeurons\": [16,16,16,16,4]\n","           ,\"DNN.Classes\": 4\n","           ,\"Training.MaxEpoch\": 400\n","           ,\"Training.BatchSize\": 120\n","           ,\"Training.LearningRate\": 0.1\n","          }\n"],"metadata":{"id":"-fGhVtyRneUP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We choose the hyperparameter set for the current model training experiment"],"metadata":{"id":"5QwJ_PBpbwOP"}},{"cell_type":"code","source":["CONFIG = CONFIG_BASELINE"],"metadata":{"id":"YnqkfSVqbuPa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Dataset loading, preprocessing and splitting\n","We create the dataset, normalize the feature values, split into training and validation set and visualizing two features.\n","\n","# Quadrapeds\n","This old dataset from 1989, generates features for 4 quadraped animals, following an normal distribution that is representative of each type of animal:\n","\n","* dogs \n","* cats \n","* horses \n","* giraffes\n","\n","Instances have 8 components: neck, four legs, torso, head, and tail.  Each component is represented as a simplified/generalized cylinder. Each cylinder is itself described by 9 features: \n","* 3D location (3 featureS)\n","* 3D axis (3 features)\n","* height, \n","* radius,\n","* texture\n","\n","Thus we have a **72-dimensional vector** for each sample to classify it to **4 classes**"],"metadata":{"id":"Cc5voM78xgaC"}},{"cell_type":"code","source":["from datasets.quadrapeds import CQuadrapedsDataSet\n","from sklearn import preprocessing\n","from mllib.visualization import CPlot\n","\n","oDataset = CQuadrapedsDataSet(1000)\n","oMinMaxScaler = preprocessing.MinMaxScaler().fit(oDataset.Samples)\n","oDataset.Samples = oMinMaxScaler.transform(oDataset.Samples)\n","print(\"Minmax normalized sample #1:\", oDataset.Samples[0])\n","oDataset.Split(0.2)\n","\n","if IS_PLOTING_DATA:\n","  # Plot the training set \n","  oPlot = CPlot(\"Dataset\", oDataset.Samples[:,6:8], oDataset.Labels\n","                ,p_sLabelDescriptions=oDataset.ClassNames, p_sColors=sColorScheme\n","                ,p_sXLabel=\"Feature 6\", p_sYLabel=\"Feature 7\"\n","                )\n","  oPlot.Show(p_bIsMinMaxScaled=False)\n","                 \n","                 \n","  # Plot the validation set\n","  oPlot = CPlot(\"Validation Set\", oDataset.VSSamples[:,6:8], oDataset.VSLabels\n","                ,p_sLabelDescriptions=oDataset.ClassNames, p_sColors=sColorScheme\n","                ,p_sXLabel=\"Feature 6\", p_sYLabel=\"Feature 7\"\n","                )\n","  oPlot.Show(p_bIsMinMaxScaled=False)\n","\n","\n","# ... Create the Tensorflow/Keras objects for feeding the data into the training algorithm\n","nBatchSize = CONFIG[\"Training.BatchSize\"]  "],"metadata":{"id":"GxOxQ9AkxYvu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### One hot encoding\n","For each sample we have a label 0 or 1 to indicate the different class. We will use two neurons for the output of the Neural Network instead of one. So each neuron should fire 1 if it detects that the sample belongs to its class. The targets for training will be changed to **one-hot encoding**."],"metadata":{"id":"F3SX-nT0cKTo"}},{"cell_type":"code","source":["tTSLabelsOnehot = tf.one_hot(oDataset.TSLabels, CONFIG[\"DNN.Classes\"])\n","tVSLabelsOnehot = tf.one_hot(oDataset.VSLabels, CONFIG[\"DNN.Classes\"])\n","\n","print(\"Labels:\",oDataset.TSLabels.shape)\n","print(\"One-hot Encoding Vectors:\", tTSLabelsOnehot.shape)\n","print(\"One-hot Target of Sample #400\", tTSLabelsOnehot[399],  \"Label:%d\" % oDataset.TSLabels[399])"],"metadata":{"id":"0NK7VS2ocKm4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Deep Neural Network Class\n","We declare the class for a **Fully Connected Deep Neural Network (FC-DNN)** that has a variable depth of hidden layers. "],"metadata":{"id":"rBjC2F3Blqs4"}},{"cell_type":"code","source":["from tensorflow import keras\n","from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n","from tensorflow.keras.layers import Activation, Softmax\\\n","\n","# =========================================================================================================================\n","class CDNNBasic(keras.Model):\n","    # --------------------------------------------------------------------------------------\n","    def __init__(self, p_oConfig):\n","        super(CDNNBasic, self).__init__(p_oConfig)\n","        # ..................... Object Attributes ...........................\n","        self.Config = p_oConfig\n","        \n","        self.ClassCount   = self.Config[\"DNN.LayerNeurons\"][-1]\n","        self.LayerNeurons = self.Config[\"DNN.LayerNeurons\"][:-1]\n","        self.HiddenLayers = [None]*len(self.LayerNeurons)\n","        self.OutputLayer  = None\n","        self.SoftmaxActivation = None\n","        self.Input        = None\n","        # ...................................................................\n","        \n","        if \"DNN.ActivationFunction\" not in self.Config:\n","            self.Config[\"DNN.ActivationFunction\"] = \"relu\"\n","                    \n","        self.Create()\n","        \n","    # --------------------------------------------------------------------------------------\n","    def Create(self):\n","        for nIndex, nLayerNeuronCount in enumerate(self.LayerNeurons):\n","            self.HiddenLayers[nIndex] = Dense(nLayerNeuronCount, activation=self.Config[\"DNN.ActivationFunction\"], use_bias=True)\n","        self.OutputLayer = Dense(self.ClassCount, use_bias=True)\n","        self.SoftmaxActivation = Softmax() \n","    # --------------------------------------------------------------------------------------\n","    def call(self, p_tInput):\n","        self.Input = p_tInput\n","        \n","        # Feed forward to the next layer\n","        tA = p_tInput\n","        for oHiddenLayer in self.HiddenLayers:\n","            tA = oHiddenLayer(tA)\n","\n","        tA = self.OutputLayer(tA)\n","        # Using the Softmax activation function for the neurons of the output layer \n","        tA = self.SoftmaxActivation(tA)\n","        \n","        return tA    \n","    # --------------------------------------------------------------------------------------\n","# ========================================================================================================================="],"metadata":{"id":"v73nWhSOdLt3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create the Neural Network model and training algorithm objects\n"],"metadata":{"id":"DhT1bvZudWDr"}},{"cell_type":"code","source":["# __________ // Create the Machine Learning model and training algorithm objects \\\\ __________\n","from DNN import CDNNBasic\n","\n","oNN = CDNNBasic(CONFIG)\n","\n","nInitialLearningRate    = CONFIG[\"Training.LearningRate\"]\n","\n","oCostFunction   = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n","oOptimizer      = tf.keras.optimizers.SGD(learning_rate=nInitialLearningRate)"],"metadata":{"id":"Lw53Davlln-5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Learning Rate Scheduling\n","We can create a method that is going to change the learning rate at specific epochs. This method receives the current lr and return the new learning rate that will be set for the algorithm. Then we create a callback Keras object."],"metadata":{"id":"U6feo-605ic4"}},{"cell_type":"code","source":["# -----------------------------------------------------------------------------------\n","def LRSchedule(epoch, lr):\n","    if epoch == 100:\n","        nNewLR = lr * 0.5\n","        print(\"Setting LR to %.5f\" % nNewLR)\n","        return nNewLR\n","    elif epoch == 200:\n","        nNewLR = lr * 0.5\n","        print(\"Setting LR to %.5f\" % nNewLR)\n","        return nNewLR\n","    elif epoch == 300:\n","        nNewLR = lr * 0.5\n","        print(\"Setting LR to %.5f\" % nNewLR)\n","        return nNewLR\n","    else:\n","        return lr\n","# -----------------------------------------------------------------------------------    \n","    \n","oLearningRateSchedule = tf.keras.callbacks.LearningRateScheduler(LRSchedule)   "],"metadata":{"id":"V6rjpfIJ5i4n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training Process\n","We call `compile()` for a Keras model object, in order for the library to prepare all the needed Tensorflow objects, a.k.a **tensor objects**. We call `fit()` on the model to execute the training process.\n","### Callbacks\n","We can provide callback objects to `fit()`, that invoke event handling methods during the training process.\n","\n","### Summary\n","After the training is finished we print a summary() that displays the count of object parameters, that are the **weights for each synapse** of our fully connected (dense) layers.\n","\n","### Saving/Loading the state\n","When the training is finished we can save the **state** of our MLP model into a folder, that is saving all the values of its weights. Then we can load the state without having to retrain our model."],"metadata":{"id":"kzI7KKYznfBi"}},{"cell_type":"code","source":["sModelFolderName = CONFIG[\"ModelName\"]\n","        \n","if (not os.path.isdir(sModelFolderName)) or IS_RETRAINING:\n","    oNN.compile(loss=oCostFunction, optimizer=oOptimizer, metrics=[\"accuracy\"])\n","\n","    if IS_DEBUGABLE:\n","        oNN.run_eagerly = True\n","        \n","    oProcessLog = oNN.fit(  oDataset.TSSamples, tTSLabelsOnehot, batch_size=nBatchSize\n","                            ,epochs=CONFIG[\"Training.MaxEpoch\"]\n","                            ,validation_data=(oDataset.VSSamples, tVSLabelsOnehot) \n","                            ,callbacks=[oLearningRateSchedule]\n","                          )\n","    oNN.summary()          \n","    oNN.save(sModelFolderName)      \n","else:\n","    # The model is trained and its state is saved (all the trainable parameters are saved). We load the model to recall the samples \n","    oNN = keras.models.load_model(sModelFolderName)\n","    oProcessLog = None\n","    oNN.summary()    "],"metadata":{"id":"3Pqd9bgmcdMx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Understanding the learning process\n","### Process Log  \n","After the training is finished the `fit()` methods returns a Python dictionary that keeps values for the error and the metrics for each training epoch."],"metadata":{"id":"HdqAw2xpfXz0"}},{"cell_type":"code","source":["if oProcessLog is not None: # [PYTHON] Checks that object reference is not Null\n","    # list all data in history\n","    print(\"Keys of Keras training process log:\", oProcessLog.history.keys())\n","    \n","    # Plot the accuracy during the training epochs\n","    plt.plot(oProcessLog.history['accuracy'])\n","    plt.plot(oProcessLog.history['val_accuracy'])\n","    plt.title('MLP Accuracy')\n","    plt.ylabel('Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.legend(['train', 'test'], loc='upper left')\n","    plt.show()\n","    \n","    # Plot the error during the training epochs\n","    sCostFunctionNameParts = oCostFunction.name.split(\"_\")                           # [PYTHON]: Splitting string into an array of strings\n","    sCostFunctionNameParts = [x.capitalize() + \" \" for x in sCostFunctionNameParts]  # [PYTHON]: List comprehension example \n","    sCostFunctionName = \" \".join(sCostFunctionNameParts)                             # [PYTHON]: Joining string in a list with the space between them\n","    \n","    \n","    plt.plot(oProcessLog.history['loss'])\n","    plt.plot(oProcessLog.history['val_loss'])\n","    plt.title('MLP ' + sCostFunctionName + \" Error\")\n","    plt.ylabel('Error')\n","    plt.xlabel('Epoch')\n","    plt.legend(['train', 'test'], loc='upper left')\n","    plt.show()"],"metadata":{"id":"2vwXe5nBfUtP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Visualizing the Neural Activations\n","We will try to understand the transformation done by the hidden layer in an MLP by visualizing in 2D a combination of its neurons. This will shed some light on how the **Universal Approximation Theorem** works in practice."],"metadata":{"id":"kXYjELg3qXkF"}},{"cell_type":"code","source":["if IS_PLOTING_DATA :\n","    HIDDEN_LAYER_INDEX = 0\n","\n","    # Plot the validation set\n","    oPlot = CPlot(\"Training Set Input Features\", oDataset.TSSamples[:,6:8], oDataset.TSLabels\n","                  ,p_sLabelDescriptions=oDataset.ClassNames, p_sColors=sColorScheme \n","                  ,p_sXLabel=\"Feature 6\", p_sYLabel=\"Feature 7\" \n","                  )\n","    oPlot.Show(p_bIsMinMaxScaled=False)\n","    \n","    \n","    tActivation = oNN.HiddenLayers[HIDDEN_LAYER_INDEX](oDataset.TSSamples)\n","    nTSSamplesTransformed = tActivation.numpy()\n","    sTitle = \"Training Set Layer %d Hidden Neuron Activations\" % (HIDDEN_LAYER_INDEX+1)\n","\n","    # Plot the validation set\n","    oPlot = CPlot(sTitle, nTSSamplesTransformed[:,:2], oDataset.TSLabels\n","                  ,p_sLabelDescriptions=oDataset.ClassNames, p_sColors=sColorScheme                  \n","                  ,p_sXLabel=\"Neuron 1\", p_sYLabel=\"Neuron 2\" )\n","    oPlot.Show(p_bIsMinMaxScaled=False)\n","\n","    if nTSSamplesTransformed.shape[1] > 2:    \n","        oPlot = CPlot(sTitle, nTSSamplesTransformed[:,1:3], oDataset.TSLabels\n","                      ,p_sLabelDescriptions=oDataset.ClassNames, p_sColors=sColorScheme                      \n","                      ,p_sXLabel=\"Neuron 2\", p_sYLabel=\"Neuron 3\" )\n","        oPlot.Show(p_bIsMinMaxScaled=False)\n","        \n","        oPlot = CPlot(sTitle, nTSSamplesTransformed[:,2:4], oDataset.TSLabels\n","                      ,p_sLabelDescriptions=oDataset.ClassNames, p_sColors=sColorScheme\n","                      ,p_sXLabel=\"Neuron 3\", p_sYLabel=\"Neuron 4\" )\n","        oPlot.Show(p_bIsMinMaxScaled=False)\n"],"metadata":{"id":"cCNTF-X0qE7e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Recalling Samples and Predicting their Class\n","We can use `predict()` on a trained model to generate its output. Having two neurons (one for each class) with their values between 0 and 1, we can consider that they output **propabilities** for the sample belonging to a class."],"metadata":{"id":"WxBn3-bHgblc"}},{"cell_type":"code","source":["nPredictedProbabilities = oNN.predict(oDataset.VSSamples)\n","nPredictedClassLabels  = np.argmax(nPredictedProbabilities, axis=1)\n","\n","nTargetClassLabels     = oDataset.VSLabels   \n","\n","for nIndex, nProbs in enumerate(nPredictedProbabilities):\n","  print(\"#%.2d Predicted:%d (Probabilities:%s) Actual:%d\" % (nIndex+1, nPredictedClassLabels[nIndex], nProbs, nTargetClassLabels[nIndex])) # [PYTHON] Format string example\n","  if nIndex == 0:\n","    print(\"Sum of all output neuron activations:%.3f\" % np.sum(nProbs))\n"],"metadata":{"id":"XkLjdDwGgh6d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Evaluation\n","We evaluate a model using different metrics and also a **confusion matrix**. These help us understand if our model works properly"],"metadata":{"id":"B2aLiFfZgi7U"}},{"cell_type":"code","source":["from mllib.evaluation import CEvaluator\n","from mllib.visualization import CPlotConfusionMatrix\n","\n","# We create an evaluator object that will produce several metrics\n","oEvaluator = CEvaluator(nTargetClassLabels, nPredictedClassLabels)\n","\n","oEvaluator.PrintConfusionMatrix()\n","print(\"Per Class Recall (Accuracy)  :\", oEvaluator.Recall)\n","print(\"Per Class Precision          :\", oEvaluator.Precision)\n","print(\"Average Accuracy: %.4f\" % oEvaluator.AverageRecall)\n","print(\"Average F1 Score: %.4f\" % oEvaluator.AverageF1Score)\n","      \n","oConfusionMatrixPlot = CPlotConfusionMatrix(oEvaluator.ConfusionMatrix)\n","oConfusionMatrixPlot.Show()      \n"],"metadata":{"id":"UUPSe4dXgbx7"},"execution_count":null,"outputs":[]}]}