{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"5.2 MLP for Binary Classification with Keras.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMLhwkbsa8xLNMV60KQzoV4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 5.3 Training of an MLP Neural Network with Tensorflow / Keras\n","This examples illustrates a basic MLP Neural Network in Tensorflow/Keras that is trained for a binary classification task. It shows different uses of  Gradient Descent optimation that achieves learning through error backpropagation:\n","* Gradient Descent: **Whole** dataset at each step\n","* Fully Stochastic Gradient Descent: **One** sample at each step\n","* Stochastic Minibatch Gradient Descent: A **minibatch** with some samples at each step.\n","\n","\n","\n"],"metadata":{"id":"DojV2IqZcaom"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"I0m8s-KccaJp"},"outputs":[],"source":["# Mount GDrive, change directory and check contents of folder.\n","\n","import os\n","from google.colab import drive\n","from google.colab import files\n","\n","PROJECT_FOLDER = \"/content/gdrive/My Drive/Colab Notebooks/CS345_SP22/5. DNN\"\n","\n","drive.mount('/content/gdrive/')\n","os.chdir(PROJECT_FOLDER)\n","print(\"Current dir: \", os.getcwd())"]},{"cell_type":"markdown","source":["# Settings and Basic Package Imports"],"metadata":{"id":"y0TJgTCXbfrZ"}},{"cell_type":"code","source":["import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow import keras\n","from mllib.utils import RandomSeed\n","\n","# __________ | Settings | __________\n","IS_PLOTING_DATA         = True\n","IS_DEBUGABLE            = False\n","IS_RETRAINING           = True\n","RandomSeed(2022)"],"metadata":{"id":"kIrldDF0bgGt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Hyperparameters\n","For each training experiment, we define all the model/training hyperparameters inside a Python dictionary."],"metadata":{"id":"5z0Jm3SFxJWv"}},{"cell_type":"code","source":["# __________ | Hyperparameters | __________\n","CONFIG_GD_LOCAL_MINIMA = {\n","            \"ModelName\": \"MLP1_LOCAL_MINIMA\"  \n","           ,\"MLP.InputFeatures\": 2\n","           ,\"MLP.HiddenNeurons\": 2\n","           ,\"MLP.Classes\": 2\n","           ,\"Training.MaxEpoch\": 40\n","           ,\"Training.BatchSize\": 160\n","           ,\"Training.LearningRate\": 0.1\n","           \n","          }\n","\n","CONFIG_GD_GOOD = { \n","            \"ModelName\": \"MLP1_TRAIN2\" \n","           ,\"MLP.InputFeatures\": 2\n","           ,\"MLP.HiddenNeurons\": 2\n","           ,\"MLP.Classes\": 2\n","           ,\"Training.MaxEpoch\": 18\n","           ,\"Training.BatchSize\": 160\n","           ,\"Training.LearningRate\": 0.1\n","          }\n","\n","CONFIG_FULLY_STOCHASTIC_GD_GOOD = {  \n","            \"ModelName\": \"MLP1_TRAIN3\"\n","           ,\"MLP.InputFeatures\": 2\n","           ,\"MLP.HiddenNeurons\": 2\n","           ,\"MLP.Classes\": 2\n","           ,\"Training.MaxEpoch\": 18\n","           ,\"Training.BatchSize\": 1\n","           ,\"Training.LearningRate\": 0.1\n","          }\n","\n","CONFIG_STOCHASTIC_MINIBATCH_GD_GOOD = {\n","            \"ModelName\": \"MLP1_TRAIN4\"  \n","           ,\"MLP.InputFeatures\": 2\n","           ,\"MLP.HiddenNeurons\": 2\n","           ,\"MLP.Classes\": 2\n","           ,\"Training.MaxEpoch\": 18\n","           ,\"Training.BatchSize\": 10\n","           ,\"Training.LearningRate\": 0.15\n","          }\n","\n","\n","CONFIG_STOCHASTIC_MINIBATCH_GD_EXCELLENT = {\n","            \"ModelName\": \"MLP1_BEST\"  \n","           ,\"MLP.InputFeatures\": 2\n","           ,\"MLP.HiddenNeurons\": 2\n","           ,\"MLP.Classes\": 2\n","           ,\"Training.MaxEpoch\": 125\n","           ,\"Training.BatchSize\": 20\n","           ,\"Training.LearningRate\": 0.15\n","          }\n","\n","CONFIG_STOCHASTIC_MINIBATCH_GD_OVERFITTING = {\n","           \"ModelName\": \"MLP1_OVERFIT\"  \n","           ,\"MLP.InputFeatures\": 2\n","           ,\"MLP.HiddenNeurons\": 2\n","           ,\"MLP.Classes\": 2\n","           ,\"Training.MaxEpoch\": 200\n","           ,\"Training.BatchSize\": 20\n","           ,\"Training.LearningRate\": 0.15\n","          }\n","\n","CONFIG_STOCHASTIC_MINIBATCH_GD_UNDERFITTING = {\n","            \"ModelName\": \"MLP1_UNDERFIT\"  \n","           ,\"MLP.InputFeatures\": 2\n","           ,\"MLP.HiddenNeurons\": 2\n","           ,\"MLP.Classes\": 2\n","           ,\"Training.MaxEpoch\": 36\n","           ,\"Training.BatchSize\": 20\n","           ,\"Training.LearningRate\": 0.15\n","          }\n","\n","CONFIG_MLP_4HIDDEN = {\n","            \"ModelName\": \"MLP1_4HIDDEN\"  \n","           ,\"MLP.InputFeatures\": 2\n","           ,\"MLP.HiddenNeurons\": 4\n","           ,\"MLP.Classes\": 2\n","           ,\"Training.MaxEpoch\": 125\n","           ,\"Training.BatchSize\": 20\n","           ,\"Training.LearningRate\": 0.15\n","          }\n","\n","CONFIG_MLP_32HIDDEN = {\n","            \"ModelName\": \"MLP1_32HIDDEN\"  \n","           ,\"MLP.InputFeatures\": 2\n","           ,\"MLP.HiddenNeurons\": 32\n","           ,\"MLP.Classes\": 2\n","           ,\"Training.MaxEpoch\": 125\n","           ,\"Training.BatchSize\": 20\n","           ,\"Training.LearningRate\": 0.15\n","          }\n"],"metadata":{"id":"-fGhVtyRneUP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We choose the hyperparameter set for the current model training experiment"],"metadata":{"id":"5QwJ_PBpbwOP"}},{"cell_type":"code","source":["CONFIG = CONFIG_STOCHASTIC_MINIBATCH_GD_UNDERFITTING"],"metadata":{"id":"YnqkfSVqbuPa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Dataset loading, preprocessing and splitting\n","We create the dataset, normalize the feature values, split into training and validation set and visualizing two features."],"metadata":{"id":"Cc5voM78xgaC"}},{"cell_type":"code","source":["# __________ // Create the data objects \\\\ __________\n","from datasets.randomdataset import CRandomDataset\n","from sklearn import preprocessing\n","from mllib.visualization import CPlot\n","\n","oDataset = CRandomDataset(p_nSampleCount=200,p_nClustersPerClass=2,p_nClassSeperability=0.7)\n","oMinMaxScaler = preprocessing.MinMaxScaler().fit(oDataset.Samples)\n","oDataset.Samples = oMinMaxScaler.transform(oDataset.Samples)\n","print(\"Minmax normalized sample #1:\", oDataset.Samples[0])\n","oDataset.Split(0.2)\n","\n","if IS_PLOTING_DATA:\n","  # Plot the training set \n","  oPlot = CPlot(\"Dataset\", oDataset.Samples, oDataset.Labels)\n","  oPlot.Show(p_bIsMinMaxScaled=False)\n","\n","  # Plot the validation set\n","  oPlot = CPlot(\"Validation Set\", oDataset.VSSamples, oDataset.VSLabels)\n","  oPlot.Show(p_bIsMinMaxScaled=False)\n","\n","# ... Create the Tensorflow/Keras objects for feeding the data into the training algorithm\n","nBatchSize = CONFIG[\"Training.BatchSize\"]"],"metadata":{"id":"GxOxQ9AkxYvu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### One hot encoding\n","For each sample we have a label 0 or 1 to indicate the different class. We will use two neurons for the output of the Neural Network instead of one. So each neuron should fire 1 if it detects that the sample belongs to its class. The targets for training will be changed to **one-hot encoding**:\n","* 0 -> [1,0]\n","* 1 -> [0,1]\n","\n","If there was a third class that has the label 2 then the one-hot encoding vectors will have 3 values:\n","\n","* 0 -> [1,0,0]\n","* 1 -> [0,1,0]\n","* 2 -> [0,0,1]"],"metadata":{"id":"F3SX-nT0cKTo"}},{"cell_type":"code","source":["tTSLabelsOnehot = tf.one_hot(oDataset.TSLabels, CONFIG[\"MLP.Classes\"])\n","tVSLabelsOnehot = tf.one_hot(oDataset.VSLabels, CONFIG[\"MLP.Classes\"])\n","\n","print(\"Labels:\",oDataset.TSLabels.shape)\n","print(\"One-hot Encoding Vectors:\", tTSLabelsOnehot.shape)\n","print(\"One-hot Target of Sample #1\", tTSLabelsOnehot[0])"],"metadata":{"id":"0NK7VS2ocKm4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# MLP Neural Network Class\n","We declare the class for a Multilayer Perceptron Neural Network that has only two layers the hidden and the output. Then we create the object for the MLP."],"metadata":{"id":"rBjC2F3Blqs4"}},{"cell_type":"code","source":["from tensorflow import keras\n","from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n","from tensorflow.keras.layers import Activation, Softmax \n","\n","# =========================================================================================================================\n","class CMLPNeuralNetwork(keras.Model):\n","    # --------------------------------------------------------------------------------------\n","    def __init__(self, p_oConfig):\n","        super(CMLPNeuralNetwork, self).__init__(p_oConfig)\n","        # ..................... Object Attributes ...........................\n","        self.Config = p_oConfig\n","        \n","        self.HiddenLayer = None\n","        self.OutputLayer = None\n","        \n","        self.Input       = None\n","        # ...................................................................\n","        \n","        if \"MLP.ActivationFunction\" not in self.Config:\n","            self.Config[\"MLP.ActivationFunction\"] = \"sigmoid\"\n","                    \n","        self.Create()\n","        \n","    # --------------------------------------------------------------------------------------\n","    def Create(self):\n","        self.HiddenLayer = Dense(self.Config[\"MLP.HiddenNeurons\"], activation=self.Config[\"MLP.ActivationFunction\"], use_bias=True)\n","        self.OutputLayer = Dense(self.Config[\"MLP.Classes\"]      , activation=self.Config[\"MLP.ActivationFunction\"], use_bias=True)\n","    # --------------------------------------------------------------------------------------\n","    def call(self, p_tInput):\n","        self.Input = p_tInput\n","        \n","        tA = self.HiddenLayer(p_tInput)\n","        tA = self.OutputLayer(tA)\n","        \n","        return tA    \n","    # --------------------------------------------------------------------------------------\n","# ========================================================================================================================="],"metadata":{"id":"v73nWhSOdLt3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create the Neural Network model and training algorithm objects\n","We place the Python class for our model into the `MLP.py` file and we simply create the model's object here"],"metadata":{"id":"DhT1bvZudWDr"}},{"cell_type":"code","source":["# __________ // Create the Machine Learning model and training algorithm objects \\\\ __________\n","from MLP import CMLPNeuralNetwork\n","oNN = CMLPNeuralNetwork(CONFIG)"],"metadata":{"id":"Lw53Davlln-5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We create an object for the error (cost) function and an object for the training algorithm, a.k.a. **the optimizer**. We will experiment with different error functions using the Stochastic Gradient Descent optimizer."],"metadata":{"id":"o9WkxBsOdmho"}},{"cell_type":"code","source":["nInitialLearningRate    = CONFIG[\"Training.LearningRate\"]\n","\n","#oCostFunction   = tf.keras.losses.MeanAbsoluteError() \n","#oCostFunction   = tf.keras.losses.MeanSquaredError()\n","oCostFunction   = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n","\n","oOptimizer      = tf.keras.optimizers.SGD(learning_rate=nInitialLearningRate)"],"metadata":{"id":"Rk20GM7XdmBE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training Process\n","We call `compile()` for a Keras model object, in order for the library to prepare all the needed Tensorflow objects, a.k.a **tensor objects**. We call `fit()` on the model to execute the training process.\n","\n","### Summary\n","After the training is finished we print a summary() that displays the count of object parameters, that are the **weights for each synapse** of our fully connected (dense) layers.\n","\n","### Saving/Loading the state\n","When the training is finished we can save the **state** of our MLP model into a folder, that is saving all the values of its weights. Then we can load the state without having to retrain our model."],"metadata":{"id":"kzI7KKYznfBi"}},{"cell_type":"code","source":["sModelFolderName = CONFIG[\"ModelName\"]\n","        \n","if (not os.path.isdir(sModelFolderName)) or IS_RETRAINING:\n","    oNN.compile(loss=oCostFunction, optimizer=oOptimizer, metrics=[\"accuracy\"])\n","\n","    if IS_DEBUGABLE:\n","        oNN.run_eagerly = True\n","        \n","    oProcessLog = oNN.fit(  oDataset.TSSamples, tTSLabelsOnehot, batch_size=nBatchSize\n","                            ,epochs=CONFIG[\"Training.MaxEpoch\"]\n","                            ,validation_data=(oDataset.VSSamples, tVSLabelsOnehot) \n","                          )\n","    oNN.summary()          \n","    oNN.save(sModelFolderName)      \n","else:\n","    # The model is trained and its state is saved (all the trainable parameters are saved). We load the model to recall the samples \n","    oNN = keras.models.load_model(sModelFolderName)\n","    oProcessLog = None\n","    oNN.summary()    "],"metadata":{"id":"3Pqd9bgmcdMx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Understanding the learning process\n","### Process Log  \n","After the training is finished the `fit()` methods returns a Python dictionary that keeps values for the error and the metrics for each training epoch."],"metadata":{"id":"HdqAw2xpfXz0"}},{"cell_type":"code","source":["if oProcessLog is not None: # [PYTHON] Checks that object reference is not Null\n","    # list all data in history\n","    print(\"Keys of Keras training process log:\", oProcessLog.history.keys())\n","    \n","    # Plot the accuracy during the training epochs\n","    plt.plot(oProcessLog.history['accuracy'])\n","    plt.plot(oProcessLog.history['val_accuracy'])\n","    plt.title('MLP Accuracy')\n","    plt.ylabel('Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.legend(['train', 'test'], loc='upper left')\n","    plt.show()\n","    \n","    # Plot the error during the training epochs\n","    sCostFunctionNameParts = oCostFunction.name.split(\"_\")                           # [PYTHON]: Splitting string into an array of strings\n","    sCostFunctionNameParts = [x.capitalize() + \" \" for x in sCostFunctionNameParts]  # [PYTHON]: List comprehension example \n","    sCostFunctionName = \" \".join(sCostFunctionNameParts)                             # [PYTHON]: Joining string in a list with the space between them\n","    \n","    \n","    plt.plot(oProcessLog.history['loss'])\n","    plt.plot(oProcessLog.history['val_loss'])\n","    plt.title('MLP ' + sCostFunctionName + \" Error\")\n","    plt.ylabel('Error')\n","    plt.xlabel('Epoch')\n","    plt.legend(['train', 'test'], loc='upper left')\n","    plt.show()"],"metadata":{"id":"2vwXe5nBfUtP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Visualizing the Neural Activations\n","We will try to understand the transformation done by the hidden layer in an MLP by visualizing in 2D a combination of its neurons. This will shed some light on how the **Universal Approximation Theorem** works in practice."],"metadata":{"id":"kXYjELg3qXkF"}},{"cell_type":"code","source":["if IS_PLOTING_DATA :\n","    # Plot the validation set\n","    oPlot = CPlot(\"Training Set Input Features\", oDataset.TSSamples, oDataset.TSLabels)\n","    oPlot.Show(p_bIsMinMaxScaled=False)\n","    \n","    \n","    tActivation = oNN.HiddenLayer(oDataset.TSSamples)\n","    nTSSamplesTransformed = tActivation.numpy()\n","    \n","    \n","    # Plot the validation set\n","    oPlot = CPlot(\"Training Set Hidden Neuron Activations\", nTSSamplesTransformed[:,0:2], oDataset.TSLabels,\n","                   p_sXLabel=\"Neuron 1\", p_sYLabel=\"Neuron 2\" )\n","    oPlot.Show(p_bIsMinMaxScaled=False)\n","\n","    if nTSSamplesTransformed.shape[1] > 2:    \n","        oPlot = CPlot(\"Training Set Hidden Neuron Activations\", nTSSamplesTransformed[:,1:3], oDataset.TSLabels,\n","                       p_sXLabel=\"Neuron 2\", p_sYLabel=\"Neuron 3\" )\n","        oPlot.Show(p_bIsMinMaxScaled=False)\n","        \n","        oPlot = CPlot(\"Training Set Hidden Neuron Activations\", nTSSamplesTransformed[:,2:4], oDataset.TSLabels,\n","                       p_sXLabel=\"Neuron 3\", p_sYLabel=\"Neuron 4\" )\n","        oPlot.Show(p_bIsMinMaxScaled=False)"],"metadata":{"id":"cCNTF-X0qE7e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Recalling Samples and Predicting their Class\n","We can use `predict()` on a trained model to generate its output. Having two neurons (one for each class) with their values between 0 and 1, we can consider that they output **propabilities** for the sample belonging to a class."],"metadata":{"id":"WxBn3-bHgblc"}},{"cell_type":"code","source":["nPredictedProbabilities = oNN.predict(oDataset.VSSamples)\n","nPredictedClassLabels  = np.argmax(nPredictedProbabilities, axis=1)\n","\n","nTargetClassLabels     = oDataset.VSLabels   \n","\n","for nIndex, nProbs in enumerate(nPredictedProbabilities):\n","  print(\"#%.2d Predicted:%d (Probabilities:%s) Actual:%d\" % (nIndex+1, nPredictedClassLabels[nIndex], nProbs, nTargetClassLabels[nIndex])) # [PYTHON] Format string example"],"metadata":{"id":"XkLjdDwGgh6d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Evaluation\n","We evaluate a model using different metrics and also a **confusion matrix**. These help us understand if our model works properly"],"metadata":{"id":"B2aLiFfZgi7U"}},{"cell_type":"code","source":["from mllib.evaluation import CEvaluator\n","\n","\n","# We create an evaluator object that will produce several metrics\n","oEvaluator = CEvaluator(nPredictedClassLabels, nTargetClassLabels)\n","\n","print(\"------------- Confusion Matrix  -----------------\")\n","print(oEvaluator.ConfusionMatrix)\n","print(\"\")\n","print(\"------------- Per Class Metrics ----------------\")\n","print(\"Per Class Recall (Accuracy)  :\", oEvaluator.Recall)\n","print(\"Per Class Precision          :\", oEvaluator.Precision)\n","print(\"\")\n","print(\"------------- Average Metrics  -----------------\")\n","print(\"AverageF1Score: %.2f\" % oEvaluator.AverageF1Score)\n","print(\"\")"],"metadata":{"id":"UUPSe4dXgbx7"},"execution_count":null,"outputs":[]}]}