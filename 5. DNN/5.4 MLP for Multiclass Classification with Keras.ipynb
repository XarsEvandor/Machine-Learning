{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"5.4 MLP for Multiclass Classification with Keras.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMTuq/G6QViyT0m2Mk9Ui1/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 5.4 Training of an MLP Neural Network for multiclass classification\n","This examples illustrates a basic MLP Neural Network in Tensorflow/Keras that is trained for a multiclass classification task. It shows different uses of  Gradient Descent optimation that achieves learning through error backpropagation:\n","* Gradient Descent: **Whole** dataset at each step\n","* Fully Stochastic Gradient Descent: **One** sample at each step\n","* Stochastic Minibatch Gradient Descent: A **minibatch** with some samples at each step."],"metadata":{"id":"DojV2IqZcaom"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"I0m8s-KccaJp"},"outputs":[],"source":["# Mount GDrive, change directory and check contents of folder.\n","\n","import os\n","from google.colab import drive\n","from google.colab import files\n","\n","PROJECT_FOLDER = \"/content/gdrive/My Drive/Colab Notebooks/CS345_SP22/5. DNN\"\n","\n","drive.mount('/content/gdrive/')\n","os.chdir(PROJECT_FOLDER)\n","print(\"Current dir: \", os.getcwd())"]},{"cell_type":"markdown","source":["# Settings and Basic Package Imports"],"metadata":{"id":"y0TJgTCXbfrZ"}},{"cell_type":"code","source":["import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow import keras\n","from mllib.utils import RandomSeed\n","\n","# __________ | Settings | __________\n","IS_PLOTING_DATA         = True\n","IS_DEBUGABLE            = False\n","IS_RETRAINING           = True\n","RandomSeed(2022)\n","\n","sColorScheme = [\"darkseagreen\", \"royalblue\", \"crimson\", \"goldenrod\"] # https://matplotlib.org/3.1.0/gallery/color/named_colors.html"],"metadata":{"id":"kIrldDF0bgGt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Hyperparameters\n","For each training experiment, we define all the model/training hyperparameters inside a Python dictionary."],"metadata":{"id":"5z0Jm3SFxJWv"}},{"cell_type":"code","source":["# __________ | Hyperparameters | __________\n","CONFIG_TRY1 = {\n","            \"ModelName\": \"QPEDS1_BASELINE\"  \n","           ,\"MLP.InputFeatures\": 72\n","           ,\"MLP.HiddenNeurons\": 72\n","           ,\"MLP.Classes\": 4\n","           ,\"Training.MaxEpoch\": 200\n","           ,\"Training.BatchSize\": 80\n","           ,\"Training.LearningRate\": 0.1\n","          }"],"metadata":{"id":"-fGhVtyRneUP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We choose the hyperparameter set for the current model training experiment"],"metadata":{"id":"5QwJ_PBpbwOP"}},{"cell_type":"code","source":["CONFIG = CONFIG_TRY1"],"metadata":{"id":"YnqkfSVqbuPa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Dataset loading, preprocessing and splitting\n","We create the dataset, normalize the feature values, split into training and validation set and visualizing two features.\n","\n","# Quadrapeds\n","This old dataset from 1989, generates features for 4 quadraped animals, following an normal distribution that is representative of each type of animal:\n","\n","* dogs \n","* cats \n","* horses \n","* giraffes\n","\n","Instances have 8 components: neck, four legs, torso, head, and tail.  Each component is represented as a simplified/generalized cylinder. Each cylinder is itself described by 9 features: \n","* 3D location (3 featureS)\n","* 3D axis (3 features)\n","* height, \n","* radius,\n","* texture\n","\n","Thus we have a **72-dimensional vector** for each sample to classify it to **4 classes**"],"metadata":{"id":"Cc5voM78xgaC"}},{"cell_type":"code","source":["from datasets.quadrapeds import CQuadrapedsDataSet\n","from sklearn import preprocessing\n","from mllib.visualization import CPlot\n","\n","oDataset = CQuadrapedsDataSet(1000)\n","oMinMaxScaler = preprocessing.MinMaxScaler().fit(oDataset.Samples)\n","oDataset.Samples = oMinMaxScaler.transform(oDataset.Samples)\n","print(\"Minmax normalized sample #1:\", oDataset.Samples[0])\n","oDataset.Split(0.2)\n","\n","if IS_PLOTING_DATA:\n","  # Plot the training set \n","  oPlot = CPlot(\"Dataset\", oDataset.Samples[:,6:8], oDataset.Labels\n","                ,p_sLabelDescriptions=oDataset.ClassNames, p_sColors=sColorScheme\n","                ,p_sXLabel=\"Feature 6\", p_sYLabel=\"Feature 7\"\n","                )\n","  oPlot.Show(p_bIsMinMaxScaled=False)\n","                 \n","                 \n","  # Plot the validation set\n","  oPlot = CPlot(\"Validation Set\", oDataset.VSSamples[:,6:8], oDataset.VSLabels\n","                ,p_sLabelDescriptions=oDataset.ClassNames, p_sColors=sColorScheme\n","                ,p_sXLabel=\"Feature 6\", p_sYLabel=\"Feature 7\"\n","                )\n","  oPlot.Show(p_bIsMinMaxScaled=False)\n","\n","\n","# ... Create the Tensorflow/Keras objects for feeding the data into the training algorithm\n","nBatchSize = CONFIG[\"Training.BatchSize\"]  "],"metadata":{"id":"GxOxQ9AkxYvu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### One hot encoding\n","For each sample we have a label 0 or 1 to indicate the different class. We will use two neurons for the output of the Neural Network instead of one. So each neuron should fire 1 if it detects that the sample belongs to its class. The targets for training will be changed to **one-hot encoding**:\n","* 0 -> [1,0]\n","* 1 -> [0,1]\n","\n","If there was a third class that has the label 2 then the one-hot encoding vectors will have 3 values:\n","\n","* 0 -> [1,0,0]\n","* 1 -> [0,1,0]\n","* 2 -> [0,0,1]"],"metadata":{"id":"F3SX-nT0cKTo"}},{"cell_type":"code","source":["tTSLabelsOnehot = tf.one_hot(oDataset.TSLabels, CONFIG[\"MLP.Classes\"])\n","tVSLabelsOnehot = tf.one_hot(oDataset.VSLabels, CONFIG[\"MLP.Classes\"])\n","\n","print(\"Labels:\",oDataset.TSLabels.shape)\n","print(\"One-hot Encoding Vectors:\", tTSLabelsOnehot.shape)\n","print(\"One-hot Target of Sample #400\", tTSLabelsOnehot[399],  \"Label:%d\" % oDataset.TSLabels[399])"],"metadata":{"id":"0NK7VS2ocKm4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# MLP Neural Network Class\n","We declare the class for a Multilayer Perceptron Neural Network that has only two layers the hidden and the output. Then we create the object for the MLP."],"metadata":{"id":"rBjC2F3Blqs4"}},{"cell_type":"code","source":["from tensorflow import keras\n","from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n","from tensorflow.keras.layers import Activation, Softmax \n","\n","# =========================================================================================================================\n","class CMLPNeuralNetwork(keras.Model):\n","    # --------------------------------------------------------------------------------------\n","    def __init__(self, p_oConfig):\n","        super(CMLPNeuralNetwork, self).__init__(p_oConfig)\n","        # ..................... Object Attributes ...........................\n","        self.Config = p_oConfig\n","        \n","        self.HiddenLayer = None\n","        self.OutputLayer = None\n","        \n","        self.Input       = None\n","        # ...................................................................\n","        \n","        if \"MLP.ActivationFunction\" not in self.Config:\n","            self.Config[\"MLP.ActivationFunction\"] = \"sigmoid\"\n","                    \n","        self.Create()\n","        \n","    # --------------------------------------------------------------------------------------\n","    def Create(self):\n","        self.HiddenLayer = Dense(self.Config[\"MLP.HiddenNeurons\"], activation=self.Config[\"MLP.ActivationFunction\"], use_bias=True)\n","        self.OutputLayer = Dense(self.Config[\"MLP.Classes\"]      , activation=self.Config[\"MLP.ActivationFunction\"], use_bias=True)\n","    # --------------------------------------------------------------------------------------\n","    def call(self, p_tInput):\n","        self.Input = p_tInput\n","        \n","        tA = self.HiddenLayer(p_tInput)\n","        tA = self.OutputLayer(tA)\n","        \n","        return tA    \n","    # --------------------------------------------------------------------------------------\n","# ========================================================================================================================="],"metadata":{"id":"v73nWhSOdLt3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create the Neural Network model and training algorithm objects\n","We place the Python class for our model into the `MLP.py` file and we simply create the model's object here"],"metadata":{"id":"DhT1bvZudWDr"}},{"cell_type":"code","source":["# __________ // Create the Machine Learning model and training algorithm objects \\\\ __________\n","from MLP import CMLPNeuralNetwork\n","oNN = CMLPNeuralNetwork(CONFIG)"],"metadata":{"id":"Lw53Davlln-5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We create an object for the error (cost) function and an object for the training algorithm, a.k.a. **the optimizer**. We will experiment with different error functions using the Stochastic Gradient Descent optimizer."],"metadata":{"id":"o9WkxBsOdmho"}},{"cell_type":"code","source":["nInitialLearningRate    = CONFIG[\"Training.LearningRate\"]\n","\n","#oCostFunction   = tf.keras.losses.MeanAbsoluteError() \n","#oCostFunction   = tf.keras.losses.MeanSquaredError()\n","oCostFunction   = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n","\n","oOptimizer      = tf.keras.optimizers.SGD(learning_rate=nInitialLearningRate)"],"metadata":{"id":"Rk20GM7XdmBE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training Process\n","We call `compile()` for a Keras model object, in order for the library to prepare all the needed Tensorflow objects, a.k.a **tensor objects**. We call `fit()` on the model to execute the training process.\n","\n","### Summary\n","After the training is finished we print a summary() that displays the count of object parameters, that are the **weights for each synapse** of our fully connected (dense) layers.\n","\n","### Saving/Loading the state\n","When the training is finished we can save the **state** of our MLP model into a folder, that is saving all the values of its weights. Then we can load the state without having to retrain our model."],"metadata":{"id":"kzI7KKYznfBi"}},{"cell_type":"code","source":["sModelFolderName = CONFIG[\"ModelName\"]\n","        \n","if (not os.path.isdir(sModelFolderName)) or IS_RETRAINING:\n","    oNN.compile(loss=oCostFunction, optimizer=oOptimizer, metrics=[\"accuracy\"])\n","\n","    if IS_DEBUGABLE:\n","        oNN.run_eagerly = True\n","        \n","    oProcessLog = oNN.fit(  oDataset.TSSamples, tTSLabelsOnehot, batch_size=nBatchSize\n","                            ,epochs=CONFIG[\"Training.MaxEpoch\"]\n","                            ,validation_data=(oDataset.VSSamples, tVSLabelsOnehot) \n","                          )\n","    oNN.summary()          \n","    oNN.save(sModelFolderName)      \n","else:\n","    # The model is trained and its state is saved (all the trainable parameters are saved). We load the model to recall the samples \n","    oNN = keras.models.load_model(sModelFolderName)\n","    oProcessLog = None\n","    oNN.summary()    "],"metadata":{"id":"3Pqd9bgmcdMx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Understanding the learning process\n","### Process Log  \n","After the training is finished the `fit()` methods returns a Python dictionary that keeps values for the error and the metrics for each training epoch."],"metadata":{"id":"HdqAw2xpfXz0"}},{"cell_type":"code","source":["if oProcessLog is not None: # [PYTHON] Checks that object reference is not Null\n","    # list all data in history\n","    print(\"Keys of Keras training process log:\", oProcessLog.history.keys())\n","    \n","    # Plot the accuracy during the training epochs\n","    plt.plot(oProcessLog.history['accuracy'])\n","    plt.plot(oProcessLog.history['val_accuracy'])\n","    plt.title('MLP Accuracy')\n","    plt.ylabel('Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.legend(['train', 'test'], loc='upper left')\n","    plt.show()\n","    \n","    # Plot the error during the training epochs\n","    sCostFunctionNameParts = oCostFunction.name.split(\"_\")                           # [PYTHON]: Splitting string into an array of strings\n","    sCostFunctionNameParts = [x.capitalize() + \" \" for x in sCostFunctionNameParts]  # [PYTHON]: List comprehension example \n","    sCostFunctionName = \" \".join(sCostFunctionNameParts)                             # [PYTHON]: Joining string in a list with the space between them\n","    \n","    \n","    plt.plot(oProcessLog.history['loss'])\n","    plt.plot(oProcessLog.history['val_loss'])\n","    plt.title('MLP ' + sCostFunctionName + \" Error\")\n","    plt.ylabel('Error')\n","    plt.xlabel('Epoch')\n","    plt.legend(['train', 'test'], loc='upper left')\n","    plt.show()"],"metadata":{"id":"2vwXe5nBfUtP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Visualizing the Neural Activations\n","We will try to understand the transformation done by the hidden layer in an MLP by visualizing in 2D a combination of its neurons. This will shed some light on how the **Universal Approximation Theorem** works in practice."],"metadata":{"id":"kXYjELg3qXkF"}},{"cell_type":"code","source":["if IS_PLOTING_DATA :\n","    # Plot the validation set\n","    oPlot = CPlot(\"Training Set Input Features\", oDataset.TSSamples[:,6:8], oDataset.TSLabels\n","                  ,p_sLabelDescriptions=oDataset.ClassNames, p_sColors=sColorScheme \n","                  ,p_sXLabel=\"Feature 6\", p_sYLabel=\"Feature 7\" \n","                  )\n","    oPlot.Show(p_bIsMinMaxScaled=False)\n","    \n","    \n","    tActivation = oNN.HiddenLayer(oDataset.TSSamples)\n","    nTSSamplesTransformed = tActivation.numpy()\n","    \n","    # Plot the validation set\n","    oPlot = CPlot(\"Training Set Hidden Neuron Activations\", nTSSamplesTransformed[:,:2], oDataset.TSLabels\n","                  ,p_sLabelDescriptions=oDataset.ClassNames, p_sColors=sColorScheme                  \n","                  ,p_sXLabel=\"Neuron 1\", p_sYLabel=\"Neuron 2\" )\n","    oPlot.Show(p_bIsMinMaxScaled=False)\n","\n","    if nTSSamplesTransformed.shape[1] > 2:    \n","        oPlot = CPlot(\"Training Set Hidden Neuron Activations\", nTSSamplesTransformed[:,1:3], oDataset.TSLabels\n","                      ,p_sLabelDescriptions=oDataset.ClassNames, p_sColors=sColorScheme                      \n","                      ,p_sXLabel=\"Neuron 2\", p_sYLabel=\"Neuron 3\" )\n","        oPlot.Show(p_bIsMinMaxScaled=False)\n","        \n","        oPlot = CPlot(\"Training Set Hidden Neuron Activations\", nTSSamplesTransformed[:,2:4], oDataset.TSLabels\n","                      ,p_sLabelDescriptions=oDataset.ClassNames, p_sColors=sColorScheme\n","                      ,p_sXLabel=\"Neuron 3\", p_sYLabel=\"Neuron 4\" )\n","        oPlot.Show(p_bIsMinMaxScaled=False)\n"],"metadata":{"id":"cCNTF-X0qE7e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Recalling Samples and Predicting their Class\n","We can use `predict()` on a trained model to generate its output. Having two neurons (one for each class) with their values between 0 and 1, we can consider that they output **propabilities** for the sample belonging to a class."],"metadata":{"id":"WxBn3-bHgblc"}},{"cell_type":"code","source":["nPredictedProbabilities = oNN.predict(oDataset.VSSamples)\n","nPredictedClassLabels  = np.argmax(nPredictedProbabilities, axis=1)\n","\n","nTargetClassLabels     = oDataset.VSLabels   \n","\n","for nIndex, nProbs in enumerate(nPredictedProbabilities):\n","  print(\"#%.2d Predicted:%d (Probabilities:%s) Actual:%d\" % (nIndex+1, nPredictedClassLabels[nIndex], nProbs, nTargetClassLabels[nIndex])) # [PYTHON] Format string example"],"metadata":{"id":"XkLjdDwGgh6d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Evaluation\n","We evaluate a model using different metrics and also a **confusion matrix**. These help us understand if our model works properly"],"metadata":{"id":"B2aLiFfZgi7U"}},{"cell_type":"code","source":["from mllib.evaluation import CEvaluator\n","from mllib.visualization import CPlotConfusionMatrix\n","\n","# We create an evaluator object that will produce several metrics\n","oEvaluator = CEvaluator(nTargetClassLabels, nPredictedClassLabels)\n","\n","oEvaluator.PrintConfusionMatrix()\n","print(\"Per Class Recall (Accuracy)  :\", oEvaluator.Recall)\n","print(\"Per Class Precision          :\", oEvaluator.Precision)\n","print(\"Average Accuracy: %.4f\" % oEvaluator.AverageRecall)\n","print(\"Average F1 Score: %.4f\" % oEvaluator.AverageF1Score)\n","      \n","oConfusionMatrixPlot = CPlotConfusionMatrix(oEvaluator.ConfusionMatrix)\n","oConfusionMatrixPlot.Show()     \n"],"metadata":{"id":"UUPSe4dXgbx7"},"execution_count":null,"outputs":[]}]}