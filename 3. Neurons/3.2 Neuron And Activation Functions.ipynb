{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3.2 Neuron And Activation Functions.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPA/3oRb/vxmS1UGQrDMrOz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 3.2 Implementing an Object-Oriented Neuron\n","This examples illustrates how to turn the mathematical model of a neuron into a software objects, with Object-Oriented programming."],"metadata":{"id":"DojV2IqZcaom"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"I0m8s-KccaJp"},"outputs":[],"source":["# Mount GDrive, change directory and check contents of folder.\n","\n","import os\n","from google.colab import drive\n","from google.colab import files\n","\n","PROJECT_FOLDER = \"/content/gdrive/My Drive/Colab Notebooks/CS345_SP22/3. Neurons\"\n","\n","drive.mount('/content/gdrive/')\n","os.chdir(\"/content/gdrive/My Drive/Colab Notebooks/CS345_SP22/3. Neurons\")\n","print(\"Current dir: \", os.getcwd())"]},{"cell_type":"markdown","source":["# Declaration of a Neuron Class\n","#### [NeuralNets]\n","* Custom implementation of synaptic integration. \n","* Custom implementation of activation functions.\n","* Custom implementation of parameter updates.\n","\n","#### [Python]\n","* Class (static) fields\n","* Random initialization of numpy arrays\n","* Default constructor parameters\n","* Private, public and protected (by convention) methods\n","* Elif chain to serve a switch statement\n","* Inheritance\n","* Creating a series of number in a given interval\n","* Plotting a function with matplotlib"],"metadata":{"id":"mVYw0MNJccNN"}},{"cell_type":"code","source":["import numpy as np\n","\n","# ====================================================================================================\n","class CNeuron(object):\n","  IS_USING_LINEAR_ALGEBRA = False\n","    \n","  # --------------------------------------------------------------------------------------\n","  # Constructor\n","  def __init__(self, p_nDendriteCount, p_sActivationFunction=\"binarystep\"):\n","    # ................................................................\n","    self.DendriteCount = p_nDendriteCount\n","    self.ActivationFunction = p_sActivationFunction\n","    \n","    self.Input          = None\n","    self.Output         = None\n","    self.weights        = np.random.rand((self.DendriteCount)).astype(np.float32)  # Random initialization of numpy array\n","    self.bias           = 0.0\n","    # ................................................................\n","  # --------------------------------------------------------------------------------------\n","  # [PYTHON] There is no implementation of protected visibility in Python, only private and public. Small letter is a convention for a protected member   \n","  def linearActivationFunction(self, u, a=1.0, b=0.0):\n","    return a*u+b; # transfers all accumulated energy from previous neurons to the next one (no actual decision, just weighting)    \n","  # --------------------------------------------------------------------------------------\n","  def stepActivationFunction(self, u):  \n","    if u >= 0:\n","        return 1.0  # fires 1, when the amount of energy surpases the threshold \n","    else:\n","        return 0.0  # silent\n","  # --------------------------------------------------------------------------------------\n","  def stepOneMinusOneActivationFunction(self, u):  \n","    if u >= 0:\n","        return 1.0  # fires an excitatory signal 1 (positive)\n","    else:\n","        return -1.0 # fires an inhibitory signal -1 (negative)\n","  # --------------------------------------------------------------------------------------\n","  def sigmoidActivationFunction(self, u):\n","    y = 1/(1 + np.exp(-u)) \n","    return y\n","  # --------------------------------------------------------------------------------------\n","  def rectifiedLinearUnitActivationFunction(self, u):\n","    if u >= 0:\n","        return u  # fires the exact amount of incoming energy, when the amount of energy surpases the threshold \n","    else:\n","        return 0.0  # silent\n","    return y\n","  # --------------------------------------------------------------------------------------\n","  # [PYTHON] There are only private and public visibilities in Python.\n","  #          When you put a double underscore as a prefix of member name it becomes private.     \n","  def __synapticIntegration(self):\n","    nSum = 0.0;\n","    for i in range(0, self.Input.shape[0]):\n","        nSum += self.weights[i]*self.Input[i]\n","\n","    return nSum\n","  # --------------------------------------------------------------------------------------\n","  def Recall(self, x):\n","    self.Input = x\n","\n","    if CPerceptron.IS_USING_LINEAR_ALGEBRA:\n","        u = np.dot(self.weights, self.Input)\n","    else:\n","        u = self.__synapticIntegration()\n","    \n","    if self.ActivationFunction == \"linear\":\n","        a = self.linearActivationFunction(u - self.bias)\n","    elif self.ActivationFunction == \"binarystep\":\n","        a = self.stepActivationFunction(u - self.bias)\n","    elif self.ActivationFunction == \"step\":\n","        a = self.stepOneMinusOneActivationFunction(u - self.bias)\n","    elif self.ActivationFunction == \"tanh\":\n","        a = np.tanh(u - self.bias)\n","    elif self.ActivationFunction == \"sigmoid\":\n","        a = self.sigmoidActivationFunction(u - self.bias)\n","    elif self.ActivationFunction == \"relu\":\n","        a = self.rectifiedLinearUnitActivationFunction(u - self.bias)    \n","\n","    self.Output = a\n","    return a;\n","  # --------------------------------------------------------------------------------------\n","  def TrainPerceptron(self, p_nLearningRate, p_nError):\n","    # Each weight is modified by an amount that is proportional to the error\n","    for nIndex in range(0, self.DendriteCount):\n","        self.weights[nIndex] = self.weights[nIndex] + p_nLearningRate*p_nError*self.Input[nIndex]\n","  # --------------------------------------------------------------------------------------        \n","# ====================================================================================================\n","class CPerceptron(CNeuron):\n","  # --------------------------------------------------------------------------------------\n","  # Constructor\n","  def __init__(self, p_nDendriteCount):\n","    super(CPerceptron, self).__init__(p_nDendriteCount, \"binarystep\")\n","# ====================================================================================================  \n"],"metadata":{"id":"3Pqd9bgmcdMx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Neuron Activation Functions\n","#### [Python]\n","How to preview the output of a function using a range of input values\n","\n","\n","### Linear and step functions"],"metadata":{"id":"6FG_XlE6lFtz"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","oNeuron = CPerceptron(2)\n","\n","\n","nXSeries = np.linspace(-10, 10, 100, dtype=np.float32)   # Create 100 real values in the space [-10.0, 10.0]\n","\n","\n","plt.title(\"Linear y=x function\")  \n","y1 = oNeuron.linearActivationFunction(nXSeries)\n","plt.plot(nXSeries, y1) \n","plt.xlabel(\"x\") \n","plt.ylabel(\"y\") \n","plt.show() \n","\n","plt.title(\"Binary step function\")  \n","y2 = np.zeros(100, np.float32)\n","for nIndex, x in enumerate(nXSeries):   # [PYTHON] Enumerate each single value and get this in pair with its index\n","  y2[nIndex] = oNeuron.stepActivationFunction(x)\n","plt.plot(nXSeries, y2) \n","plt.xlabel(\"x\") \n","plt.ylabel(\"y\") \n","plt.show() \n","\n","plt.title(\"Step one minus one function\")  \n","y3 = np.zeros(100, np.float32)\n","for nIndex, x in enumerate(nXSeries):   # [PYTHON] Enumerate each single value and get this in pair with its index\n","  y3[nIndex] = oNeuron.stepOneMinusOneActivationFunction(x)\n","plt.plot(nXSeries, y3) \n","plt.xlabel(\"x\") \n","plt.ylabel(\"y\") \n","plt.show() \n","  "],"metadata":{"id":"o_rzTppalGG9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Sigmoidal functions and ReLU"],"metadata":{"id":"YgX5NBjtyYPF"}},{"cell_type":"code","source":["import numpy as np\n","\n","plt.title(\"Sigmoid function\")  \n","y4 = oNeuron.sigmoidActivationFunction(nXSeries)\n","plt.plot(nXSeries, y4) \n","plt.xlabel(\"x\") \n","plt.ylabel(\"y\") \n","plt.show() \n","\n","plt.title(\"Tanh function\")  \n","y4 = np.tanh(nXSeries)\n","plt.plot(nXSeries, y4) \n","plt.xlabel(\"x\") \n","plt.ylabel(\"y\") \n","plt.show() \n","\n","\n","plt.title(\"ReLU function\")  \n","y5 = np.zeros(100, np.float32)\n","for nIndex, x in enumerate(nXSeries):   # [PYTHON] Enumerate each single value and get this in pair with its index\n","  y5[nIndex] = oNeuron.rectifiedLinearUnitActivationFunction(x)  \n","plt.plot(nXSeries, y5) \n","plt.xlabel(\"x\") \n","plt.ylabel(\"y\") \n","plt.show() "],"metadata":{"id":"hxAbJSHylgm9"},"execution_count":null,"outputs":[]}]}