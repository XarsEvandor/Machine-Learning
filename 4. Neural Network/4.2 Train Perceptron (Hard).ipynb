{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4.2 Train Perceptron (Hard).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOU4uuRajnFG9xWHHQxrDUH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 4.2 Training of a Single Perceptron Neuron on non-separable 2D Data\n","This examples illustrates a basic Machine Learning algorithm for a classification task and a simplistic model that learns its parameters on samples with only 2 features.\n","\n"],"metadata":{"id":"DojV2IqZcaom"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"I0m8s-KccaJp"},"outputs":[],"source":["# Mount GDrive, change directory and check contents of folder.\n","\n","import os\n","from google.colab import drive\n","from google.colab import files\n","\n","PROJECT_FOLDER = \"/content/gdrive/My Drive/Colab Notebooks/CS345_SP22/4. Neural Network\"\n","\n","drive.mount('/content/gdrive/')\n","os.chdir(PROJECT_FOLDER)\n","print(\"Current dir: \", os.getcwd())"]},{"cell_type":"markdown","source":["# Prepare dataset object, model object and define all hyperparameters\n","* There are hyperparameters with regard to the data, like the count of features\n","* There are hyperparameters for the model, in the simplest one-neuron model only the count of inputs that is the same with the count of features in the data.\n","* There are hyperparameters for training, like the count of iterations (epoch) of the algorithm.\n"],"metadata":{"id":"mVYw0MNJccNN"}},{"cell_type":"code","source":["from Dataset import CRandomDataset\n","from Neuron import CPerceptron\n","\n","FEATURE_COUNT = 2       # Samples are 2D vectors, because there are 2 features\n","\n","# ... // Create the data objects \\\\ ...\n","oDataset = CRandomDataset(p_nSampleCount=200,p_nClustersPerClass=2,p_nClassSeperability=0.7)\n","oDataset.Split(0.1)\n","\n","# ... // Create the ML model \\\\ ...\n","oNeuron = CPerceptron(p_nDendriteCount = FEATURE_COUNT)\n","print(\"Perceptron model parameters (initial values):\", oNeuron.weights, oNeuron.bias)\n","\n","# Hyperparameters\n","MAX_EPOCH = 100;\n","LEARNING_RATE = 1e-3;"],"metadata":{"id":"-fGhVtyRneUP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Training loop"],"metadata":{"id":"kzI7KKYznfBi"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from mllib.visualization import CPlot\n","\n","oMAETraining = []\n","\n","# ... // Main loop for supervised training, implementing a ML algorithm \\\\ ...\n","nEpochNumber = 0;\n","bContinueTraining = True\n","while bContinueTraining:\n","  nEpochNumber += 1\n","    \n","  # Recall samples through the model to calculate error\n","  nSampleError      = np.zeros((oDataset.TSSampleCount), dtype=np.float32)\n","  nSumError         = 0.0\n","  nMeanAbsoluteError = 0.0\n"," \n","  for nIndex in range(0, oDataset.TSSampleCount):   # [C# to PYTHON]: for(int nIndex = 0; nIndex < oDataset.TSSampleCount; nIndex++)\n","    # 1 sample at each step\n","    nSample = oDataset.TSSamples[nIndex]\n","    t = oDataset.TSLabels[nIndex]   # target for training\n","    y = oNeuron.Recall(nSample);    # ground truth label for training\n","    nError = t-y                    # cost (error) function\n","\n","    # We adjust the model for all samples according to the summed error -> This is learning!\n","    oNeuron.TrainPerceptron(LEARNING_RATE, nError)\n","\n","    nSampleError[nIndex] = nError\n","    nSumError += nError\n","    nMeanAbsoluteError = np.sum(np.abs(nSampleError[:nIndex + 1]))/(nIndex + 1)  #  [PYTHON]: It contains a slicing operation of nSampleError vector from 0 to nIndex\n","    if (((nIndex % 50) == 0) or (nIndex == oDataset.TSSampleCount)): # print every 50 samples\n","        print(\" |___        Sample: [%3d] Sample error:%.6f  MAE Error:%.6f\" % (nIndex, nError, nMeanAbsoluteError))\n","\n","  \n","\n","  # Keep some stats to show\n","  oMAETraining.append(nMeanAbsoluteError)\n","\n","  print(\"Epoch: [%3d] Mean Absolute Error:%.6f\" % (nEpochNumber, nMeanAbsoluteError))\n","\n","  # Termination condition for training loop -> Don't stuck in an infinite training loop when there is nothing more to learn\n","  if (bContinueTraining):\n","    bContinueTraining = (nEpochNumber < MAX_EPOCH) # Simple condition when reaching a maximum of epochs"],"metadata":{"id":"3Pqd9bgmcdMx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Understanding the learning process\n","We can understand if our model learns, by inspecting the value of error that should be decreasing in each epoch, reaching a point that it cannot learn more."],"metadata":{"id":"kXYjELg3qXkF"}},{"cell_type":"code","source":["# Plot the error after the training is complete\n","oTrainingError = np.asarray(oMAETraining, dtype=np.float32)\n","plt.plot(oMAETraining)\n","plt.show()\n"],"metadata":{"id":"mbLZ7asBqUR9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Understanding the functionality of classification\n","We can understand classification done with a single neuron in the 2D space through visualization. The perceptron tries to draw a line that separates the samples of different classes."],"metadata":{"id":"T_3Tw9vnqzMC"}},{"cell_type":"code","source":["print(\"Perceptron model parameters (learned values):\", oNeuron.weights, oNeuron.bias)\n","\n","nSlope = -oNeuron.weights[0] / oNeuron.weights[1]\n","nIntercept = -oNeuron.bias / oNeuron.weights[1]\n","\n","\n","# Plot the decision line on the whole dataset\n","oPlot = CPlot(\"Dataset\", oDataset.Samples, oDataset.Labels)\n","oPlot.Show(p_bIsMinMaxScaled=False, p_nLineSlope=nSlope, p_nLineIntercept=nIntercept)\n","\n","# Plot the decision line on the validation set\n","oPlot = CPlot(\"Validation Set\", oDataset.VSSamples, oDataset.VSLabels)\n","oPlot.Show(p_bIsMinMaxScaled=False, p_nLineSlope=nSlope, p_nLineIntercept=nIntercept)"],"metadata":{"id":"cCNTF-X0qE7e"},"execution_count":null,"outputs":[]}]}