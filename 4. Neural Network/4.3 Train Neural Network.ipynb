{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4.3 Train Neural Network.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMojHYrM02EDHSGBNXpyhYw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 4.3 Training of a Neural Network on non-separable 2D Data\n","This examples illustrates a basic MLP Neural Network that is trained for a binary classification task using Gradient Descent. It learns its parameters on samples that have only 2 features.\n","\n"],"metadata":{"id":"DojV2IqZcaom"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"I0m8s-KccaJp"},"outputs":[],"source":["# Mount GDrive, change directory and check contents of folder.\n","\n","import os\n","from google.colab import drive\n","from google.colab import files\n","\n","PROJECT_FOLDER = \"/content/gdrive/My Drive/Colab Notebooks/CS345_SP22/4. Neural Network\"\n","\n","drive.mount('/content/gdrive/')\n","os.chdir(PROJECT_FOLDER)\n","print(\"Current dir: \", os.getcwd())"]},{"cell_type":"markdown","source":["# Hyperparameters\n","We define the constant values for the architectural hyperparameters (neuron counts) and the training hyperparameters (epochs, learning rate)."],"metadata":{"id":"5z0Jm3SFxJWv"}},{"cell_type":"code","source":["  \n","# _____ | Settings | ______\n","IS_PLOTING_DATA         = True\n","\n","# _____ | Hyperparameters | ______\n","#// Architectural \\\\\n","INPUT_FEATURES              = 2\n","HIDDEN_NEURONS              = 2\n","#HIDDEN_NEURONS              = 3\n","#HIDDEN_NEURONS              = 5\n","#HIDDEN_NEURONS              = 8\n","\n","# // Learning \\\\\n","MAX_EPOCH                   = 100;\n","LEARNING_RATE               = 0.01;"],"metadata":{"id":"-fGhVtyRneUP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Dataset loading, preprocessing and splitting\n","We create the dataset, normalize the values of the features, split into training and validation set."],"metadata":{"id":"Cc5voM78xgaC"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn import preprocessing\n","\n","from mllib.visualization import CPlot\n","from Dataset import CRandomDataset\n","\n","\n","# ... // Create the data objects \\\\ ...\n","oDataset = CRandomDataset(p_nSampleCount=200,p_nClustersPerClass=2,p_nClassSeperability=0.7)\n","oMinMaxScaler = preprocessing.MinMaxScaler().fit(oDataset.Samples)\n","oDataset.Samples = oMinMaxScaler.transform(oDataset.Samples)\n","print(\"Minmax normalized sample #1:\", oDataset.Samples[0])\n","oDataset.Split(0.2)\n","\n","if IS_PLOTING_DATA:\n","    # Plot the training set \n","    oPlot = CPlot(\"Dataset\", oDataset.Samples, oDataset.Labels)\n","    oPlot.Show(p_bIsMinMaxScaled=False)\n","\n","    # Plot the validation set\n","    oPlot = CPlot(\"Validation Set\", oDataset.VSSamples, oDataset.VSLabels)\n","    oPlot.Show(p_bIsMinMaxScaled=False)"],"metadata":{"id":"GxOxQ9AkxYvu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# MLP Neural Network Class\n","We declare the class for a Multilayer Perceptron Neural Network that has only two layers the hidden and the output. Then we create the object for the MLP."],"metadata":{"id":"rBjC2F3Blqs4"}},{"cell_type":"code","source":["\n","from Layer import CNeuralLayer\n","\n","from ActivationFunctions import Relu, ReluDerivative, Sigmoid, SigmoidDerivative\n","\n","# ====================================================================================================\n","class CMLPNeuralNetwork(list):\n","  # --------------------------------------------------------------------------------------\n","  # Constructor\n","  def __init__(self):\n","    # ................................................................\n","    self.__lastActivations = []\n","    self.Input       = None\n","    self.HiddenLayer = None\n","    self.OutputLayer = None\n","    self.Output      = None\n","    # ................................................................\n","    self.Create()\n","  # --------------------------------------------------------------------------------------\n","  def __call__(self, p_oInput):\n","      # Input Layer\n","      self.Input = p_oInput\n","      \n","      return self.Recall()\n","  # --------------------------------------------------------------------------------------\n","  def Create(self):\n","    # ... // Create the ML model \\\\ ...\n","    self.HiddenLayer = CNeuralLayer(HIDDEN_NEURONS, INPUT_FEATURES)\n","    self.OutputLayer = CNeuralLayer(1, HIDDEN_NEURONS)\n","    \n","    self.append(self.HiddenLayer)\n","    self.append(self.OutputLayer)\n","  # --------------------------------------------------------------------------------------\n","  def Recall(self):\n","    self.__lastActivations = []\n","\n","    # Hidden layer\n","    u1 = self.HiddenLayer(self.Input)\n","    a1 = []\n","    for nIndex, u in enumerate(u1): \n","        a = Sigmoid(u)\n","        a1.append(a)\n","    self.__lastActivations.append(a1)\n","    \n","    # Output layer\n","    u2 = self.OutputLayer(a1)\n","    a2 = []\n","    for nIndex, u in enumerate(u2): \n","        a = Sigmoid(u)\n","        a2.append(a)\n","    self.__lastActivations.append(a2)\n","\n","    y = a2\n","\n","    return y\n","  # --------------------------------------------------------------------------------------\n","  def BackPropagateError(self, p_nError): # easy to read example for 2 layers\n","    a1, a2 = self.__lastActivations  # [PYTHON] Unpacking of multiple values from a list or a tuple\n","\n","    # Calculate for the output layer\n","    vDelta2 = np.zeros((self.OutputLayer.NeuronCount), np.float64)\n","    for nThisNeuronIndex, oNeuron in enumerate(self.OutputLayer):\n","      vDelta2[nThisNeuronIndex] = SigmoidDerivative(a2[nThisNeuronIndex])*p_nError\n","      \n","    # Calculate for the previous layer (hidden layer)\n","    vDelta1 = np.zeros((self.HiddenLayer.NeuronCount), np.float64)\n","    for nThisNeuronIndex, oNeuron in enumerate(self.HiddenLayer):\n","      \n","      nBackPropagatedError = 0\n","      for nNextNeuronIndex, oNextNeuron in enumerate(self.OutputLayer):\n","        nWeight = oNextNeuron.weights[nThisNeuronIndex]  # This is the synaptic weight between oNeuron and oNextNeuron\n","        nBackPropagatedError += nWeight * vDelta2[nNextNeuronIndex]\n","      \n","      #vDelta1[nThisNeuronIndex] = sigmoidDerivative(a1[nThisNeuronIndex])*nBackPropagatedError\n","      vDelta1[nThisNeuronIndex] = SigmoidDerivative(a1[nThisNeuronIndex])*nBackPropagatedError\n","      \n","    oDeltas = []    # [PYTHON] List constructor\n","    oDeltas.append(vDelta1)\n","    oDeltas.append(vDelta2)\n","\n","    return oDeltas\n","  # --------------------------------------------------------------------------------------\n","  def UpdateWeights(self, p_nLearningRate, p_nDeltas): # easy to read example for 2 layers\n","    for nLayerIndex,oLayer in enumerate(self):\n","      for nNeuronIndex, oNeuron in enumerate(oLayer):\n","          oNeuron.TrainGradientDescent(p_nLearningRate, p_nDeltas[nLayerIndex][nNeuronIndex])\n","  # --------------------------------------------------------------------------------------\n","# ====================================================================================================\n","\n","\n","# ... // Create the ML model \\\\ ...\n","oNN = CMLPNeuralNetwork()"],"metadata":{"id":"Lw53Davlln-5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Training loop"],"metadata":{"id":"kzI7KKYznfBi"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","\n","oMeanError = []\n","\n","# ... // Main loop for supervised training, implementing a ML algorithm \\\\ ...\n","nEpochNumber = 0;\n","bContinueTraining = True\n","while bContinueTraining:\n","  nEpochNumber += 1\n","\n","  # Recall samples through the model to calculate error\n","  nPerSampleLoss = np.zeros((oDataset.TSSampleCount), dtype=np.float64)\n","\n","  # We recall the whole training set\n","  y_true = []\n","  y_pred = []\n","  \n","  for nIndex in range(0, oDataset.TSSampleCount):\n","    # 1 sample at each step (this is Fully Stochastic Gradient Descent)\n","    nSample  = oDataset.TSSamples[nIndex]\n","    t = oDataset.TSLabels[nIndex]   # target for training\n","    y_true.append(t)\n","\n","    # 1. RECALL\n","    nOutput = oNN(nSample)\n","    y = nOutput[0]\n","    if y >= 0.5:\n","        nPredictedClass = 1.0\n","    else:\n","        nPredictedClass = 0.0\n","    y_pred.append(nPredictedClass)\n","    \n","    # 2 CALCULATE ERROR \n","    nSampleLoss = y - t  # The error of each sample is called loss\n","    nPerSampleLoss[nIndex] = nSampleLoss\n","    \n","    # 3. BACKPROPAGE ERROR AND CALCULATE GRADIENTS\n","    oDeltas = oNN.BackPropagateError(nSampleLoss)  # [PYTHON] Parentheses create a tuple that is a collection of objects\n","    \n","    # 4. UPDATE WEIGHTS USING GRADIENTS\n","    oNN.UpdateWeights(LEARNING_RATE, oDeltas)\n","    \n","    \n","  \n","  nTrainingSetMeanError = np.asarray(nPerSampleLoss).mean()\n","  oMeanError.append(nTrainingSetMeanError)\n","\n","  nTrainingAccuracy = accuracy_score(y_true, y_pred)*100.0\n","\n","  # Evaluating the model accuracy with the samples that are not shown during training\n","  y_true = []\n","  y_pred = []\n","  for nIndex in range(0, oDataset.VSSampleCount):   \n","    nSample  = oDataset.VSSamples[nIndex]\n","    t = oDataset.VSLabels[nIndex]\n","    y_true.append(t)\n","    \n","    nOutput = oNN(nSample)\n","    y = nOutput[0]\n","    if y >= 0.5:\n","        nPredictedClass = 1.0\n","    else:\n","        nPredictedClass = 0.0\n","    y_pred.append(nPredictedClass)\n","    \n","  nValidationAccuracy = accuracy_score(y_true, y_pred)*100.0 \n","\n","  \n","  # Keep some stats to show\n","  \n","  print(\"Epoch: [%3d] | {MSE} TRN:%.6f | {ACCURACY} TRN:%.4f%% VAL:%.4f%%\" % (nEpochNumber, nTrainingSetMeanError, nTrainingAccuracy, nValidationAccuracy))\n","\n","  # Termination condition for training loop -> Don't stuck in an infinite training loop when there is nothing more to learn\n","  if (bContinueTraining):\n","    bContinueTraining = (nEpochNumber < MAX_EPOCH) # Simple condition when reaching a maximum of epochs\n"],"metadata":{"id":"3Pqd9bgmcdMx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Understanding the learning process\n","We can understand if our model learns, by inspecting the value of error that should be decreasing in each epoch, reaching a point that it cannot learn more."],"metadata":{"id":"kXYjELg3qXkF"}},{"cell_type":"code","source":["# Plot the error after the training is complete\n","oTrainingError = np.asarray(oMeanError, dtype=np.float64)\n","plt.plot(oMeanError)\n","plt.show()\n"],"metadata":{"id":"cCNTF-X0qE7e"},"execution_count":null,"outputs":[]}]}