{"cells":[{"cell_type":"markdown","metadata":{"id":"KUGvmm_yn1uo"},"source":["# 8.1 Creating a complex CNN architecture and use advanced Deep Learning training techniques\n","This examples illustrates a more advanced CNN architecture in Tensorflow/Keras, with the use of **custom Keras layers** that implement complex convolutional modules. It is trained on the well-known CIFAR10 dataset (32x32 color images of 10 classes), using a data feed pipeline that augments the dataset with new random samples.\n","\n","\n"," Except the usual convolutional and max pooling layer it incorporatens the **Global Average Pooling** layer before the **Logits Dense Layer** that are the current approach for the classifier part of a CNN."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k2taBUomqJHh"},"outputs":[],"source":["# Mount GDrive, change directory and check contents of folder.\n","\n","import os\n","from google.colab import drive\n","from google.colab import files\n","\n","PROJECT_FOLDER = \"/content/gdrive/My Drive/Colab Notebooks/CS345_SP22/6. CNN\"\n","\n","drive.mount('/content/gdrive/')\n","os.chdir(PROJECT_FOLDER)\n","print(\"Current dir: \", os.getcwd())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A1uCq93lnzWS"},"outputs":[],"source":["import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow import keras\n","from mllib.utils import RandomSeed\n","\n","# __________ | Settings | __________\n","IS_PLOTING_DATA         = True\n","IS_DEBUGABLE            = False\n","IS_RETRAINING           = True\n","RandomSeed(2022)"]},{"cell_type":"markdown","source":["# Hyperparameters\n","For each training experiment, we define all the model/training hyperparameters inside a Python dictionary.\n","\n","\n","**Deep Learning Techniques**:\n","\n","* L2 Regularization with a given weight decay will try to keep small weights preventing overfitting. It can be applied to specific layers.\n","* Momentum can be using during training to include the error from the previous epoch.\n","* Learning Rate scheduling, a list that contains paris of [epoch,lr]. We set learning rate at a specific value at a given epoch\n"],"metadata":{"id":"5z0Jm3SFxJWv"}},{"cell_type":"code","source":["CONFIG_CUSTOM_CNN = {\n","                 \"ModelName\": \"CIFAR10_MyCustomCNN\"\n","                ,\"CNN.InputShape\": [32,32,3]\n","                ,\"CNN.Classes\": 10\n","                ,\"CNN.ConvOutputFeatures\": [32,32,32,32,32]\n","                ,\"Training.MaxEpoch\": 28\n","                ,\"Training.BatchSize\": 128\n","                ,\"Training.LearningRate\": 0.1\n","                ,\"Training.LearningRateScheduling\": [[10,0.05]]\n","                ,\"Training.Momentum\": 0.9    \n","                ,\"Training.RegularizeL2\": False\n","                ,\"Training.WeightDecay\": 1e-4\n","            }\n","                \n","CONFIG = CONFIG_CUSTOM_CNN\n","                     "],"metadata":{"id":"-fGhVtyRneUP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We choose the hyperparameter set for the current model training experiment"],"metadata":{"id":"5QwJ_PBpbwOP"}},{"cell_type":"code","source":["CONFIG = CONFIG_CUSTOM_CNN"],"metadata":{"id":"YnqkfSVqbuPa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# CIFAR10\n","The [CIFAR10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) dataset, that dates back to 2009, has become a standard toy dataset to understand the image classification task. It contains 60000 tiny color images of 32x32 resolution for the classes\n","1. airplane, 2. automobile, 3. bird, 4. cat , 5. deer, 6. dog, 7. frog, 8. horse, 9. ship, 10. truck\n","                               \n","It is already splitted into a training set of 50000 images (5000 images per class), while the rest 10000 (1000 images per classe) are used to validate the model\n","\n","# Dataset loading\n","We are using a custom implementation of the dataset that downloads and converts the images into Python pickle files. "],"metadata":{"id":"Cc5voM78xgaC"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"uiDspBvJoHOZ"},"outputs":[],"source":["from datasets.cifar10.dataset import CCIFAR10DataSet\n","\n","# ... // Create the data objects \\\\ ...\n","oDataset = CCIFAR10DataSet()\n","print(\"Training samples set shape:\", oDataset.TSSamples.shape)\n","print(\"Validation samples set shape:\", oDataset.VSSamples.shape)"]},{"cell_type":"markdown","source":["# Data Feeding for Training and Validation\n","We create two data feeding objects that speed-up the loading of samples to the model's input. We use the `tf.data.Dataset` class and its method `from_tensor_slices()` to create the data feed object supplying as parameters the numpy arrays for the features of all TS samples `oDataset.TSSamples` and the corresponding class labels `oDataset.TSLabels`. We do the same for the VS.\n"],"metadata":{"id":"VPyYbsBWY-nb"}},{"cell_type":"code","source":["# -----------------------------------------------------------------------------\n","def __normalizeImage(p_tImage):\n","    # Normalizes color component values from `uint8` to `float32`.\n","    return tf.cast(p_tImage, tf.float32) / 255.\n","# ------------------------------------------------------------------------------\n","def PreprocessImageAugmentDataset(p_tImageInTS, p_tLabelInTS):\n","    # Normalizes color component values from `uint8` to `float32`.\n","    tNormalizedImage = __normalizeImage(p_tImageInTS)\n","    # Calls the data augmentation function that add new random samples, i.e.augments the dataset. \n","    tNewRandomImage = tf.image.random_flip_left_right(tNormalizedImage)\n","    \n","    # Target class labels into one-hot encoding\n","    tTargetOneHot = tf.one_hot(p_tLabelInTS, CONFIG[\"CNN.Classes\"])\n","    \n","    return tNewRandomImage, tTargetOneHot\n","# -----------------------------------------------------------------------------------\n","nBatchSize = CONFIG[\"Training.BatchSize\"]\n","\n","# Training data feed pipeline\n","oTSData = tf.data.Dataset.from_tensor_slices((oDataset.TSSamples, oDataset.TSLabels))\n","oTSData = oTSData.map(PreprocessImageAugmentDataset, num_parallel_calls=tf.data.AUTOTUNE)\n","oTSData = oTSData.cache()\n","oTSData = oTSData.shuffle(oDataset.TSSampleCount)\n","oTSData = oTSData.batch(nBatchSize)\n","print(\"Will shuffle all the %d samples in the TS before splitting into batches with %d samples/batch\" % (oDataset.TSSampleCount, nBatchSize))\n","\n","\n","\n","\n","# Validation data feed pipeline\n","# -----------------------------------------------------------------------------------\n","def PreprocessImage(p_tImageInVS, p_tLabelInVS):\n","    # Normalizes color component values from `uint8` to `float32`.\n","    tNormalizedImage = __normalizeImage(p_tImageInVS)\n","    # Target class labels into one-hot encoding\n","    tTargetOneHot = tf.one_hot(p_tLabelInVS, CONFIG[\"CNN.Classes\"])\n","    \n","    return tNormalizedImage, tTargetOneHot\n","# -----------------------------------------------------------------------------------\n","oVSData = tf.data.Dataset.from_tensor_slices((oDataset.VSSamples, oDataset.VSLabels))\n","oVSData = oVSData.map(PreprocessImage, num_parallel_calls=tf.data.AUTOTUNE)\n","oVSData = oVSData.batch(oDataset.VSSampleCount)\n","print(\"One batch for all the %d samples in the VS\" % oDataset.VSSampleCount)\n","\n","print(\".\"*50)\n","print(\"Training data feed object:\", oTSData)\n","print(\"Validation data feed object:\", oVSData)"],"metadata":{"id":"fSWlxinCY9lG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For training there is a pre-processing pipeline:\n","* We normalize each sample in the scale [0,1]\n","* We do a random left-right flip to create a new random sample, out of the original one.\n","* We cache the TS (for speed)\n","* We shuffle the TS.\n","* We split the TS into minibatches, that will be fed input the model's input in each training steps of the epoch. \n","\n","For validation we must not do the random augmentation, no shuffling is needed but must do the same normalization.\n","\n","\n","**Deep Learning techniques**: The training pipeline implements \n","\n","* Input feature normalization.\n","* Dataset augmentation.\n","* Random minibatch sampling out of the available TS samples.\n"],"metadata":{"id":"o7K8HX5baaiL"}},{"cell_type":"markdown","source":["# Custom Complex CNN Architecture\n","We will declare a `keras.Model` descendant class to implement our complex CNN. We will create convolutional modules, from `keras.Layer` descendants that can contain multiple keras layers, which complex connections.\n","\n","**Deep Learning Techniques**:\n","\n","* We choose a proper weight initialization method, here `glorot_uniform`. You can try with `he_normal`.\n","* The convolutional kernels inside the modules are regularized.\n","* We are using the **DropOut** technique. We will randomly keep only a fraction of the input neurons (e.g. 60%) for the logits inside a training step. The rest 40% of the neurons are not trained. In the next step a different 60% of the neurons are trained.This mitigates overfitting because we have fewer neurons to train."],"metadata":{"id":"P_tiMbyzc1jV"}},{"cell_type":"code","source":["# __________ // Create the Machine Learning model and training algorithm objects \\\\ __________\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras import regularizers\n","from models.ConvModules import CBasicConvModule\n","from mllib.helpers import CKerasModelStructure, CModelConfig\n","\n","# =========================================================================================================================\n","class CMyCustomCNN(keras.Model):\n","    # --------------------------------------------------------------------------------------\n","    # Constructor\n","    def __init__(self, p_oConfig):\n","      super(CMyCustomCNN, self).__init__()\n","      \n","      # ..................... Object Attributes ...........................\n","      self.Config = CModelConfig(self, p_oConfig)\n","      \n","      self.ClassCount         = self.Config.Value[\"CNN.Classes\"]\n","      self.ConvLayerFeatures  = self.Config.Value[\"CNN.ConvOutputFeatures\"]\n","      self.Structure = None \n","      # ......... Keras layers .........\n","      self.StemConv1              = None\n","      self.StemActivation1        = None\n","      self.StemBatchNorm1         = None\n","      \n","      self.StemConv2              = None\n","      self.StemActivation2        = None\n","      self.StemBatchNorm2         = None\n","      \n","      self.Module1                = None\n","      self.Module2                = None\n","      self.Module3                = None\n","      \n","      self.GlobalAveragePooling   = None\n","      self.DropOut                = None\n","      self.Logits                 = None\n","      self.SoftmaxActivation      = None\n","      # ...................................................................\n","      \n","      \n","      self.Create()\n","    # --------------------------------------------------------------------------------------------------------\n","    def createWeightRegulizer(self):\n","        if self.Config.Value[\"Training.RegularizeL2\"]:\n","            oWeightRegularizer = regularizers.L2(self.Config.Value[\"Training.WeightDecay\"])\n","        else:\n","            oWeightRegularizer = None\n","        return oWeightRegularizer          \n","    # --------------------------------------------------------------------------------------\n","    def Create(self): \n","        self.StemConv1        = layers.Conv2D(self.ConvLayerFeatures[0], kernel_size=(3,3), strides=1, padding=\"same\"\n","                                          , use_bias=False\n","                                          , kernel_initializer=\"glorot_uniform\"\n","                                          , bias_initializer=\"zeros\"\n","                                          , kernel_regularizer=self.createWeightRegulizer()\n","                                          )\n","        self.StemActivation1 = layers.Activation(\"relu\")\n","        self.StemBatchNorm1  = layers.BatchNormalization()\n","        \n","        \n","        self.StemConv2        = layers.Conv2D(self.ConvLayerFeatures[1], kernel_size=(3,3), strides=2, padding=\"same\"\n","                                          , use_bias=False\n","                                          , kernel_initializer=\"glorot_uniform\"\n","                                          , bias_initializer=\"zeros\"  \n","                                          , kernel_regularizer=self.createWeightRegulizer()\n","                                          )     \n","        self.StemActivation2        = layers.Activation(\"relu\")\n","        self.StemBatchNorm2         = layers.BatchNormalization()\n","        \n","\n","                \n","        # ..... PLACE YOUR CUSTOM ARCHITECTURE HERE .....\n","        oCommonModuleConfig={  \"Convolution.Features\"           : None \n","                              ,\"Convolution.PaddingSize\"        : 1\n","                              ,\"Convolution.WindowSize\"         : 3\n","                              ,\"Convolution.Stride\"             : 1\n","                              ,\"Convolution.KernelInitializer\"  : \"glorot_uniform\"\n","                              ,\"Convolution.HasBias\"            : False\n","                              ,\"Convolution.BiasInitializer\"    : None\n","                              ,\"Convolution.RegularizeL2\"       : self.Config.Value[\"Training.RegularizeL2\"]\n","                              ,\"Convolution.WeightDecay\"        : self.Config.Value[\"Training.WeightDecay\"]\n","                              ,\"ActivationFunction\"             : \"relu\"\n","                              ,\"Normalization\"                  : \"BatchNormalization\"\n","                            }\n","      \n","        # ... = CBasicConvModule(self, oCommonModuleConfig, self.ConvLayerFeatures[3],p_bIsMaxPoolDownsampling=True)\n","        \n","\n","      \n","        # Using Global Average Pooling to flatten the activation tensor into an average vector\n","        self.GlobalAveragePooling = layers.GlobalAveragePooling2D()\n","        \n","        # Using dropout to keep 60% of the neurons randomly in each step of the training process. This mitigates overfitting. \n","        self.DropOut = layers.Dropout(rate=0.4)\n","\n","        # Output layer with class neurons that will use the SoftMax activation function    \n","        self.Logits = layers.Dense(self.ClassCount\n","                                         , use_bias=True\n","                                         , kernel_initializer=\"glorot_uniform\"\n","                                         , bias_initializer=\"zeros\"\n","                                         , kernel_regularizer=self.createWeightRegulizer()\n","                                   )\n","        self.SoftmaxActivation = layers.Softmax()           \n","    # --------------------------------------------------------------------------------------------------------\n","    def call(self, p_tInput):\n","        # Lazy initialization of the model structure. Will run the logic of adding keras layer to the structure just once.\n","        bPrint = self.Structure is None\n","        if bPrint:\n","            self.Structure = CKerasModelStructure()\n","            \n","        # ....... Stem  .......\n","        tA = p_tInput\n","        if bPrint:\n","            self.Structure.Add(tA)\n","        \n","        # First learnable convolutional module\n","        tA = self.StemConv1(tA)\n","        if bPrint:\n","            self.Structure.Add(tA)\n","        \n","        tA = self.StemActivation1(tA)\n","        if bPrint:\n","            self.Structure.Add(tA)\n","        \n","        tA = self.StemBatchNorm1(tA)\n","        if bPrint:\n","            self.Structure.Add(tA)\n","        \n","        # Second learnable convolutional module\n","        tA = self.StemConv2(tA)\n","        if bPrint:\n","            self.Structure.Add(tA)\n","        \n","        tA = self.StemActivation2(tA)\n","        if bPrint:\n","            self.Structure.Add(tA)\n","        \n","        tA = self.StemBatchNorm2(tA)\n","        if bPrint:\n","            self.Structure.Add(tA)\n","        \n","        \n","        \n","        # ....... Core  .......\n","        # ..... PLACE YOUR CUSTOM ARCHITECTURE HERE .....\n","      \n","        \n","        # ....... Classifier  .......\n","        tA = self.GlobalAveragePooling(tA)\n","        if bPrint:\n","            self.Structure.Add(tA)\n","\n","        tA = self.DropOut(tA)\n","        if bPrint:\n","            self.Structure.Add(tA)\n","        \n","        tA = self.Logits(tA)\n","        if bPrint:\n","            self.Structure.Add(tA)\n","        \n","        tA = self.SoftmaxActivation(tA)\n","        if bPrint:\n","            self.Structure.Add(tA)\n","        \n","        return tA\n","    # --------------------------------------------------------------------------------------------------------\n","# ========================================================================================================================="],"metadata":{"id":"m1klKxV_c17N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create the Neural Network model and training algorithm objects\n","**Deep Learning techniques**\n","\n","*   Learning rate scheduling, for more fine-grained weight updates after some epoch\n","*   Momentum, to improve convergence and avoid local error minima.\n","\n"],"metadata":{"id":"DhT1bvZudWDr"}},{"cell_type":"code","source":["oNN = CMyCustomCNN(CONFIG)\n","\n","# -----------------------------------------------------------------------------------\n","def LRSchedule(epoch, lr):\n","    nNewLR = lr\n","    for nIndex,oSchedule in enumerate(CONFIG[\"Training.LearningRateScheduling\"]):\n","        if epoch == oSchedule[0]:\n","            nNewLR = oSchedule[1]\n","            print(\"Schedule #%d: Setting LR to %.5f\" % (nIndex+1,nNewLR))\n","            break\n","    return nNewLR\n","# -----------------------------------------------------------------------------------    \n","\n","nInitialLearningRate    = CONFIG[\"Training.LearningRate\"]  \n","  \n","\n","oCostFunction   = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n","oOptimizer = tf.keras.optimizers.SGD(learning_rate=nInitialLearningRate, momentum=CONFIG[\"Training.Momentum\"])\n","if CONFIG[\"Training.LearningRateScheduling\"] is not None:\n","  oCallbacks = [tf.keras.callbacks.LearningRateScheduler(LRSchedule)]\n","else:\n","  oCallbacks = None"],"metadata":{"id":"Lw53Davlln-5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Inspect the model architecture"],"metadata":{"id":"-sq6PkuMHRyM"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6PQlq3KEoFVB"},"outputs":[],"source":["# Compile the model for training\n","sModelFolderName = CONFIG[\"ModelName\"]\n","        \n","bIsCompiledForTraining = False\n","if not os.path.isdir(sModelFolderName) or IS_RETRAINING:\n","    oNN.compile(loss=oCostFunction, optimizer=oOptimizer, metrics=[\"accuracy\"])\n","    oNN.predict(oVSData)\n","    oNN.Structure.Print(\"Model-Structure-%s.csv\" % CONFIG[\"ModelName\"])\n","    bIsCompiledForTraining = True"]},{"cell_type":"markdown","metadata":{"id":"JUmXdT2coFHN"},"source":["### Train and evalute the model"]},{"cell_type":"code","source":["if bIsCompiledForTraining:\n","    # Train the model\n","    if IS_DEBUGABLE:\n","        oNN.run_eagerly = True\n","        \n","    oProcessLog = oNN.fit(  oTSData, batch_size=nBatchSize\n","                            ,epochs=CONFIG[\"Training.MaxEpoch\"]\n","                            ,validation_data=oVSData\n","                            ,callbacks=oCallbacks\n","                          )\n","    oNN.summary()          \n","    oNN.save(sModelFolderName)      \n","else:\n","    # The model is trained and its state is saved (all the trainable parameters are saved). We load the model to recall the samples \n","    oNN = keras.models.load_model(sModelFolderName)\n","    oProcessLog = None\n","    oNN.summary()    \n"],"metadata":{"id":"GyPhQqKB2HXr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Learning Process Overview"],"metadata":{"id":"HdqAw2xpfXz0"}},{"cell_type":"code","source":["if oProcessLog is not None: # [PYTHON] Checks that object reference is not Null\n","    # list all data in history\n","    print(\"Keys of Keras training process log:\", oProcessLog.history.keys())\n","    \n","    # Plot the accuracy during the training epochs\n","    plt.plot(oProcessLog.history['accuracy'])\n","    plt.plot(oProcessLog.history['val_accuracy'])\n","    plt.title('CNN Accuracy')\n","    plt.ylabel('Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.legend(['train', 'test'], loc='upper left')\n","    plt.show()\n","    \n","    # Plot the error during the training epochs\n","    sCostFunctionNameParts = oCostFunction.name.split(\"_\")                           # [PYTHON]: Splitting string into an array of strings\n","    sCostFunctionNameParts = [x.capitalize() + \" \" for x in sCostFunctionNameParts]  # [PYTHON]: List comprehension example \n","    sCostFunctionName = \" \".join(sCostFunctionNameParts)                             # [PYTHON]: Joining string in a list with the space between them\n","    \n","    \n","    plt.plot(oProcessLog.history['loss'])\n","    plt.plot(oProcessLog.history['val_loss'])\n","    plt.title('CNN ' + sCostFunctionName + \" Error\")\n","    plt.ylabel('Error')\n","    plt.xlabel('Epoch')\n","    plt.legend(['train', 'test'], loc='upper left')\n","    plt.show()"],"metadata":{"id":"2vwXe5nBfUtP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Inference / Evaluation"],"metadata":{"id":"B2aLiFfZgi7U"}},{"cell_type":"code","source":["from mllib.evaluation import CEvaluator\n","from mllib.visualization import CPlotConfusionMatrix\n","\n","# Takes one minibatch that is the whole VS.\n","for tImages, tLabels in oVSData.take(1):\n","    nImages            = tImages.numpy()\n","    nTargetClassOneHot = tLabels.numpy()  \n","nTargetClassLabels = np.argmax(nTargetClassOneHot, axis=1)\n","\n","nPredictedProbabilities = oNN.predict(nImages)\n","nPredictedClassLabels  = np.argmax(nPredictedProbabilities, axis=1)\n","\n","oEvaluator = CEvaluator(nTargetClassLabels, nPredictedClassLabels)\n","\n","oEvaluator.PrintConfusionMatrix()\n","print(\"Per Class Recall (Accuracy)  :\", oEvaluator.Recall)\n","print(\"Per Class Precision          :\", oEvaluator.Precision)\n","print(\"Average Accuracy: %.4f\" % oEvaluator.AverageRecall)\n","print(\"Average F1 Score: %.4f\" % oEvaluator.AverageF1Score)\n","      \n","oConfusionMatrixPlot = CPlotConfusionMatrix(oEvaluator.ConfusionMatrix)\n","oConfusionMatrixPlot.Show()      \n"],"metadata":{"id":"UUPSe4dXgbx7"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"name":"8.1. Advanced Deep Learning with CNNs - CIFAR10.ipynb","provenance":[{"file_id":"1CsDNudi1KDNIXto7hjG_JECo37RbJvHk","timestamp":1611919183124}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"}},"nbformat":4,"nbformat_minor":0}